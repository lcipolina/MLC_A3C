{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3-Ray-RLLIb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y6wOpWfUgqv5",
        "ZvjU5Dr6g_p3",
        "1Hofaz0-9GOa",
        "Dvi_08jVhwsQ",
        "wkCnl2QosyH2",
        "qd_Yy_nm_vs5",
        "k94AfEtXi_QT"
      ],
      "authorship_tag": "ABX9TyM+MRu/QvK19wwL9X699Cxv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lcipolina/MLC_A3C/blob/main/Copy_of_AC3_Ray_RLLIb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZMwliNct0Iq"
      },
      "source": [
        "# Simple Demo of Ray's RLLib\n",
        "\n",
        "We show how to train a reinforcement learning environment that has been built on top of OpenAI Gym using Ray and RLlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZXg2LQOd6h5"
      },
      "source": [
        "# Importing the usuals\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import sklearn\n",
        "import json  #to convert result files into Json\n",
        "import sys, os\n",
        "\n",
        "# just to display my images\n",
        "import cv2  \n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NHtaXgYN5dD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccaf81b4-5f18-4556-9948-afff0b0bcfdf"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 448 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "\u001b[33mWARNING: gym 0.17.3 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box_2D]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufih9PL6N-GM"
      },
      "source": [
        "import os\n",
        "import Box2D\n",
        "import pyglet\n",
        "import imageio\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKFAFr32ijke",
        "outputId": "db22036f-7d32-4533-8848-e54615a02f1b"
      },
      "source": [
        "# Gym RL algos will be running under the hood\n",
        "!pip install gym\n",
        "import gym"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl_nBYVyh0OC"
      },
      "source": [
        "Rendering Gym in Colab\n",
        "\n",
        "https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab\n",
        "\n",
        "https://colab.research.google.com/drive/16gZuQlwxmxR5ZWYLZvBeq3bTdFfb1r_6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nOrz6-02Rri"
      },
      "source": [
        "### RLLib in Ray\n",
        "Ray comes with many popular DRL models already coded (they are wrapped from Gym), so we are using a “packaged” PPO from RLLIB, we don’t have to code anything ourselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-uaV3hPyaKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccbaa427-5b8c-46c4-be26-46677e2a3926"
      },
      "source": [
        "!pip install ray[rllib]\n",
        "!pip install 'ray[default]'\n",
        "\n",
        "#Warning: Given that we are executing our examples in Colab we need to restart the runtime after installing ray package"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[rllib]\n",
            "  Downloading ray-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl (54.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.0 MB 46 kB/s \n",
            "\u001b[?25hCollecting redis>=3.5.0\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 564 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.13)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.41.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.19.5)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (21.2.0)\n",
            "Collecting lz4\n",
            "  Downloading lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.1.5)\n",
            "Requirement already satisfied: matplotlib!=3.4.3 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (3.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.8.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.17.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.16.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from ray[rllib]) (0.1.6)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[rllib]) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.4.3->ray[rllib]) (0.10.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->ray[rllib]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ray[rllib]) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[rllib]) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[rllib]) (1.24.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (7.1.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->ray[rllib]) (2.6.3)\n",
            "Installing collected packages: redis, tensorboardX, ray, lz4\n",
            "Successfully installed lz4-3.1.3 ray-1.7.0 redis-3.5.3 tensorboardX-2.4\n",
            "Requirement already satisfied: ray[default] in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.5.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[default]) (21.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (7.1.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.41.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.19.5)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.17.3)\n",
            "Collecting opencensus\n",
            "  Downloading opencensus-0.8.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.23.0)\n",
            "Collecting gpustat\n",
            "  Downloading gpustat-0.6.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (0.11.0)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 49.4 MB/s \n",
            "\u001b[?25hCollecting aioredis<2\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.10-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 27.1 MB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting hiredis\n",
            "  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray[default]) (1.15.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray[default]) (3.0.4)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->ray[default]) (2.10)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray[default]) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]) (1.26.3)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2018.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.35.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (21.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (2021.5.30)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=7219d93d24722487cc67054d903c857cb8158101716194aeb24a966efdf9c5c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n",
            "Successfully built gpustat\n",
            "Installing collected packages: multidict, yarl, async-timeout, opencensus-context, hiredis, blessings, aiohttp, py-spy, opencensus, gpustat, colorful, aioredis, aiohttp-cors\n",
            "Successfully installed aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorful-0.5.4 gpustat-0.6.0 hiredis-2.0.0 multidict-5.2.0 opencensus-0.8.0 opencensus-context-0.1.2 py-spy-0.3.10 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwK0Vn391Vsm"
      },
      "source": [
        "Define directory for checkpoints\n",
        " \n",
        " Checkpoints are used for the Rollouts of the policy after training or to resume training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrXS5GjS1UtO"
      },
      "source": [
        "import shutil\n",
        "\n",
        "#Main saving directory\n",
        "CHECKPOINT_ROOT = \"tmp/ppo/cart\"\n",
        "\n",
        "# Where checkpoints are written:\n",
        "shutil.rmtree(CHECKPOINT_ROOT, ignore_errors=True, onerror=None)\n",
        "\n",
        "# Where some data will be written and used by Tensorboard below:\n",
        "ray_results = os.getenv(\"HOME\") + \"/ray_results/\"\n",
        "shutil.rmtree(ray_results, ignore_errors=True, onerror=None)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN_Z2aCqzjBe"
      },
      "source": [
        "### Initializing Ray for laptop use (without cluster)\n",
        "\n",
        "In the Ray initialization command is where we define the parallelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brLrcsmyziAc",
        "outputId": "1207cda8-9d89-43f1-e553-8fea04d62dfc"
      },
      "source": [
        "import ray\n",
        "#import ray.rllib.agents.ppo as ppo #Import RL model to use (pre-built on RLLIB)\n",
        "\n",
        "import ray.rllib.agents.a3c as a3c\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "#NOTE: It prints the dashboard running on a local port"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:08:48,255\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metrics_export_port': 61693,\n",
              " 'node_id': 'd080c3378105e8045f50ebedc5733a927925b1ce02089c9589eb50f0',\n",
              " 'node_ip_address': '172.28.0.2',\n",
              " 'object_store_address': '/tmp/ray/session_2021-10-29_18-08-46_328775_65/sockets/plasma_store',\n",
              " 'raylet_ip_address': '172.28.0.2',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2021-10-29_18-08-46_328775_65/sockets/raylet',\n",
              " 'redis_address': '172.28.0.2:6379',\n",
              " 'session_dir': '/tmp/ray/session_2021-10-29_18-08-46_328775_65',\n",
              " 'webui_url': '127.0.0.1:8265'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw_DvFOO7Fpx"
      },
      "source": [
        "### Gym Environment Configuration in RLLib\n",
        "\n",
        "RLLib supports several environments: https://docs.ray.io/en/master/rllib-env.html\n",
        "\n",
        "Here we will train a policy with PPO using Gym's Carpole environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-aYRuO7JEO"
      },
      "source": [
        "# Env Configuration\n",
        "SELECT_ENV = \"LunarLander-v2\"                      # Specifies the OpenAI Gym environment for Cart Pole\n",
        "\n",
        "# PPO parameters are passed in a Config dict\n",
        "config = a3c.DEFAULT_CONFIG.copy()              # PPO's default configuration. See the next code cell.\n",
        "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed."
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2esb8tps5hl"
      },
      "source": [
        "The general synthax for RLLib envs is to instantiate the class with parameters into a Congfig Dict\n",
        "\n",
        "```\n",
        "ray.init()\n",
        "trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
        "    \"env_config\": {},  # config to pass to env class\n",
        "      })\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMIe0GeNrWv6",
        "outputId": "23f0afbe-e1d1-4086-8391-720ba5cddc72"
      },
      "source": [
        "# Initializes the training class and object\n",
        "agent = a3c.A3CTrainer(config, env=SELECT_ENV)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:19:24,061\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-29 18:19:24,071\tINFO trainable.py:112 -- Trainable.setup took 10.454 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2021-10-29 18:19:24,075\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEDyuUZa8KDf"
      },
      "source": [
        "### Training\n",
        "Results are saved at: root/ray_results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83EuranD75JK",
        "outputId": "17140d6d-2449-437c-f470-3ae27f1b89c1"
      },
      "source": [
        "# Training\n",
        "N_ITER = 3 #only 3 iterations to show the idea   (By default, training runs for 10 iterations).\n",
        "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
        "\n",
        "for n in range(N_ITER):\n",
        "  result = agent.train()                   # each call to agent.train() returns a object containing information that we will inspect below\n",
        "  file_name = agent.save(CHECKPOINT_ROOT) \n",
        "\n",
        "  #Print training stats\n",
        "  print(s.format(\n",
        "    n + 1,\n",
        "    result[\"episode_reward_min\"],\n",
        "    result[\"episode_reward_mean\"],\n",
        "    result[\"episode_reward_max\"],\n",
        "    result[\"episode_len_mean\"],\n",
        "    file_name\n",
        "   ))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1 reward    nan/   nan/   nan len    nan saved tmp/ppo/cart/checkpoint_000001/checkpoint-1\n",
            "  2 reward -448.77/-267.71/-73.47 len  94.53 saved tmp/ppo/cart/checkpoint_000002/checkpoint-2\n",
            "  3 reward -633.34/-324.59/-73.47 len  98.20 saved tmp/ppo/cart/checkpoint_000003/checkpoint-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqbHCUAL2mTS",
        "outputId": "f53b93b6-8f45-46b0-ea3a-97b808586f1a"
      },
      "source": [
        "#https://github.com/anyscale/academy/blob/main/ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb\n",
        "\n",
        "# Similarly, we can save the training stats on list to inspect\n",
        "results = []\n",
        "episode_data = []\n",
        "episode_json = []\n",
        "\n",
        "for n in range(N_ITER):\n",
        "    result = agent.train()\n",
        "    results.append(result)\n",
        "    \n",
        "    episode = {'n': n, \n",
        "               'episode_reward_min': result['episode_reward_min'], \n",
        "               'episode_reward_mean':result['episode_reward_mean'], \n",
        "               'episode_reward_max': result['episode_reward_max'],  \n",
        "               'episode_len_mean':   result['episode_len_mean']}\n",
        "    \n",
        "    episode_data.append(episode)\n",
        "    episode_json.append(json.dumps(episode))\n",
        "    file_name = agent.save(CHECKPOINT_ROOT)\n",
        "    \n",
        "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Min/Mean/Max reward:  10.0000/ 95.9100/448.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000004/checkpoint-4\n",
            "  1: Min/Mean/Max reward:  10.0000/133.9900/500.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000005/checkpoint-5\n",
            "  2: Min/Mean/Max reward:  10.0000/167.4900/500.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000006/checkpoint-6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdrJyuKTedVL"
      },
      "source": [
        "### Inspect Training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLlL-Fx37Ipj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3458e088-16e0-4468-c32f-ac18298c6be1"
      },
      "source": [
        "# Inspect the results object\n",
        "results"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'agent_timesteps_total': 4000,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_param': 0.3,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.0,\n",
              "   'entropy_coeff_schedule': None,\n",
              "   'env': 'CartPole-v1',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': None,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'kl_coeff': 0.2,\n",
              "   'kl_target': 0.01,\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 5e-05,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 0,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [100, 50],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': False,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 0,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_sgd_iter': 10,\n",
              "   'num_workers': 1,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 200,\n",
              "   'sample_async': False,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'sgd_minibatch_size': 250,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'shuffle_sequences': True,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 4000,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_clip_param': 10.0,\n",
              "   'vf_loss_coeff': 1.0,\n",
              "   'vf_share_layers': -1},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-29_18-08-07',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 23.609467455621303,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 71.0,\n",
              "  'episode_reward_mean': 23.609467455621303,\n",
              "  'episode_reward_min': 9.0,\n",
              "  'episodes_this_iter': 169,\n",
              "  'episodes_total': 169,\n",
              "  'experiment_id': '2821ac43e99b4fb980c141f7206d3200',\n",
              "  'hist_stats': {'episode_lengths': [16,\n",
              "    18,\n",
              "    11,\n",
              "    13,\n",
              "    23,\n",
              "    23,\n",
              "    31,\n",
              "    15,\n",
              "    20,\n",
              "    15,\n",
              "    12,\n",
              "    35,\n",
              "    48,\n",
              "    18,\n",
              "    37,\n",
              "    17,\n",
              "    28,\n",
              "    20,\n",
              "    35,\n",
              "    56,\n",
              "    24,\n",
              "    10,\n",
              "    12,\n",
              "    19,\n",
              "    13,\n",
              "    38,\n",
              "    16,\n",
              "    11,\n",
              "    40,\n",
              "    27,\n",
              "    14,\n",
              "    35,\n",
              "    18,\n",
              "    71,\n",
              "    24,\n",
              "    35,\n",
              "    36,\n",
              "    52,\n",
              "    23,\n",
              "    27,\n",
              "    19,\n",
              "    19,\n",
              "    13,\n",
              "    19,\n",
              "    13,\n",
              "    30,\n",
              "    31,\n",
              "    16,\n",
              "    41,\n",
              "    34,\n",
              "    13,\n",
              "    46,\n",
              "    21,\n",
              "    22,\n",
              "    16,\n",
              "    33,\n",
              "    20,\n",
              "    26,\n",
              "    12,\n",
              "    18,\n",
              "    13,\n",
              "    26,\n",
              "    13,\n",
              "    38,\n",
              "    45,\n",
              "    18,\n",
              "    15,\n",
              "    49,\n",
              "    16,\n",
              "    29,\n",
              "    21,\n",
              "    47,\n",
              "    16,\n",
              "    20,\n",
              "    21,\n",
              "    14,\n",
              "    18,\n",
              "    20,\n",
              "    21,\n",
              "    15,\n",
              "    29,\n",
              "    25,\n",
              "    24,\n",
              "    36,\n",
              "    9,\n",
              "    12,\n",
              "    29,\n",
              "    20,\n",
              "    9,\n",
              "    21,\n",
              "    12,\n",
              "    14,\n",
              "    32,\n",
              "    26,\n",
              "    16,\n",
              "    23,\n",
              "    31,\n",
              "    23,\n",
              "    11,\n",
              "    11,\n",
              "    28,\n",
              "    35,\n",
              "    17,\n",
              "    33,\n",
              "    33,\n",
              "    21,\n",
              "    14,\n",
              "    20,\n",
              "    18,\n",
              "    34,\n",
              "    37,\n",
              "    29,\n",
              "    24,\n",
              "    15,\n",
              "    38,\n",
              "    18,\n",
              "    62,\n",
              "    12,\n",
              "    27,\n",
              "    15,\n",
              "    49,\n",
              "    16,\n",
              "    17,\n",
              "    16,\n",
              "    36,\n",
              "    45,\n",
              "    20,\n",
              "    17,\n",
              "    39,\n",
              "    30,\n",
              "    13,\n",
              "    15,\n",
              "    14,\n",
              "    24,\n",
              "    29,\n",
              "    16,\n",
              "    20,\n",
              "    24,\n",
              "    12,\n",
              "    12,\n",
              "    19,\n",
              "    20,\n",
              "    65,\n",
              "    17,\n",
              "    14,\n",
              "    13,\n",
              "    12,\n",
              "    13,\n",
              "    15,\n",
              "    19,\n",
              "    17,\n",
              "    20,\n",
              "    14,\n",
              "    22,\n",
              "    15,\n",
              "    15,\n",
              "    12,\n",
              "    35,\n",
              "    36,\n",
              "    10,\n",
              "    17,\n",
              "    18,\n",
              "    51,\n",
              "    20,\n",
              "    24,\n",
              "    23,\n",
              "    22,\n",
              "    15,\n",
              "    17],\n",
              "   'episode_reward': [16.0,\n",
              "    18.0,\n",
              "    11.0,\n",
              "    13.0,\n",
              "    23.0,\n",
              "    23.0,\n",
              "    31.0,\n",
              "    15.0,\n",
              "    20.0,\n",
              "    15.0,\n",
              "    12.0,\n",
              "    35.0,\n",
              "    48.0,\n",
              "    18.0,\n",
              "    37.0,\n",
              "    17.0,\n",
              "    28.0,\n",
              "    20.0,\n",
              "    35.0,\n",
              "    56.0,\n",
              "    24.0,\n",
              "    10.0,\n",
              "    12.0,\n",
              "    19.0,\n",
              "    13.0,\n",
              "    38.0,\n",
              "    16.0,\n",
              "    11.0,\n",
              "    40.0,\n",
              "    27.0,\n",
              "    14.0,\n",
              "    35.0,\n",
              "    18.0,\n",
              "    71.0,\n",
              "    24.0,\n",
              "    35.0,\n",
              "    36.0,\n",
              "    52.0,\n",
              "    23.0,\n",
              "    27.0,\n",
              "    19.0,\n",
              "    19.0,\n",
              "    13.0,\n",
              "    19.0,\n",
              "    13.0,\n",
              "    30.0,\n",
              "    31.0,\n",
              "    16.0,\n",
              "    41.0,\n",
              "    34.0,\n",
              "    13.0,\n",
              "    46.0,\n",
              "    21.0,\n",
              "    22.0,\n",
              "    16.0,\n",
              "    33.0,\n",
              "    20.0,\n",
              "    26.0,\n",
              "    12.0,\n",
              "    18.0,\n",
              "    13.0,\n",
              "    26.0,\n",
              "    13.0,\n",
              "    38.0,\n",
              "    45.0,\n",
              "    18.0,\n",
              "    15.0,\n",
              "    49.0,\n",
              "    16.0,\n",
              "    29.0,\n",
              "    21.0,\n",
              "    47.0,\n",
              "    16.0,\n",
              "    20.0,\n",
              "    21.0,\n",
              "    14.0,\n",
              "    18.0,\n",
              "    20.0,\n",
              "    21.0,\n",
              "    15.0,\n",
              "    29.0,\n",
              "    25.0,\n",
              "    24.0,\n",
              "    36.0,\n",
              "    9.0,\n",
              "    12.0,\n",
              "    29.0,\n",
              "    20.0,\n",
              "    9.0,\n",
              "    21.0,\n",
              "    12.0,\n",
              "    14.0,\n",
              "    32.0,\n",
              "    26.0,\n",
              "    16.0,\n",
              "    23.0,\n",
              "    31.0,\n",
              "    23.0,\n",
              "    11.0,\n",
              "    11.0,\n",
              "    28.0,\n",
              "    35.0,\n",
              "    17.0,\n",
              "    33.0,\n",
              "    33.0,\n",
              "    21.0,\n",
              "    14.0,\n",
              "    20.0,\n",
              "    18.0,\n",
              "    34.0,\n",
              "    37.0,\n",
              "    29.0,\n",
              "    24.0,\n",
              "    15.0,\n",
              "    38.0,\n",
              "    18.0,\n",
              "    62.0,\n",
              "    12.0,\n",
              "    27.0,\n",
              "    15.0,\n",
              "    49.0,\n",
              "    16.0,\n",
              "    17.0,\n",
              "    16.0,\n",
              "    36.0,\n",
              "    45.0,\n",
              "    20.0,\n",
              "    17.0,\n",
              "    39.0,\n",
              "    30.0,\n",
              "    13.0,\n",
              "    15.0,\n",
              "    14.0,\n",
              "    24.0,\n",
              "    29.0,\n",
              "    16.0,\n",
              "    20.0,\n",
              "    24.0,\n",
              "    12.0,\n",
              "    12.0,\n",
              "    19.0,\n",
              "    20.0,\n",
              "    65.0,\n",
              "    17.0,\n",
              "    14.0,\n",
              "    13.0,\n",
              "    12.0,\n",
              "    13.0,\n",
              "    15.0,\n",
              "    19.0,\n",
              "    17.0,\n",
              "    20.0,\n",
              "    14.0,\n",
              "    22.0,\n",
              "    15.0,\n",
              "    15.0,\n",
              "    12.0,\n",
              "    35.0,\n",
              "    36.0,\n",
              "    10.0,\n",
              "    17.0,\n",
              "    18.0,\n",
              "    51.0,\n",
              "    20.0,\n",
              "    24.0,\n",
              "    23.0,\n",
              "    22.0,\n",
              "    15.0,\n",
              "    17.0]},\n",
              "  'hostname': '78972b9d4096',\n",
              "  'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
              "     'learner_stats': {'cur_kl_coeff': 0.20000000298023224,\n",
              "      'cur_lr': 4.999999873689376e-05,\n",
              "      'entropy': 0.6896719,\n",
              "      'entropy_coeff': 0.0,\n",
              "      'kl': 0.0033037334,\n",
              "      'model': {},\n",
              "      'policy_loss': -0.008651653,\n",
              "      'total_loss': 246.58272,\n",
              "      'vf_explained_var': 6.22537e-05,\n",
              "      'vf_loss': 246.5907}}},\n",
              "   'num_agent_steps_sampled': 4000,\n",
              "   'num_agent_steps_trained': 4000,\n",
              "   'num_steps_sampled': 4000,\n",
              "   'num_steps_trained': 4000},\n",
              "  'iterations_since_restore': 1,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 1,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 60.339999999999996, 'ram_util_percent': 16.71},\n",
              "  'pid': 65,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.06847463825648202,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 0.08018825448056931,\n",
              "   'mean_inference_ms': 1.1948884293247062,\n",
              "   'mean_raw_obs_processing_ms': 0.1139667623491771},\n",
              "  'time_since_restore': 6.979264259338379,\n",
              "  'time_this_iter_s': 6.979264259338379,\n",
              "  'time_total_s': 6.979264259338379,\n",
              "  'timers': {'learn_throughput': 3882.517,\n",
              "   'learn_time_ms': 1030.259,\n",
              "   'load_throughput': 11008671.916,\n",
              "   'load_time_ms': 0.363,\n",
              "   'sample_throughput': 673.867,\n",
              "   'sample_time_ms': 5935.886,\n",
              "   'update_time_ms': 3.294},\n",
              "  'timestamp': 1635530887,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 4000,\n",
              "  'training_iteration': 1},\n",
              " {'agent_timesteps_total': 8000,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_param': 0.3,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.0,\n",
              "   'entropy_coeff_schedule': None,\n",
              "   'env': 'CartPole-v1',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': None,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'kl_coeff': 0.2,\n",
              "   'kl_target': 0.01,\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 5e-05,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 0,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [100, 50],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': False,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 0,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_sgd_iter': 10,\n",
              "   'num_workers': 1,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 200,\n",
              "   'sample_async': False,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'sgd_minibatch_size': 250,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'shuffle_sequences': True,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 4000,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_clip_param': 10.0,\n",
              "   'vf_loss_coeff': 1.0,\n",
              "   'vf_share_layers': -1},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-29_18-08-14',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 31.03875968992248,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 115.0,\n",
              "  'episode_reward_mean': 31.03875968992248,\n",
              "  'episode_reward_min': 9.0,\n",
              "  'episodes_this_iter': 129,\n",
              "  'episodes_total': 298,\n",
              "  'experiment_id': '2821ac43e99b4fb980c141f7206d3200',\n",
              "  'hist_stats': {'episode_lengths': [40,\n",
              "    20,\n",
              "    10,\n",
              "    26,\n",
              "    34,\n",
              "    38,\n",
              "    45,\n",
              "    22,\n",
              "    26,\n",
              "    35,\n",
              "    20,\n",
              "    48,\n",
              "    18,\n",
              "    34,\n",
              "    49,\n",
              "    15,\n",
              "    27,\n",
              "    17,\n",
              "    17,\n",
              "    54,\n",
              "    115,\n",
              "    43,\n",
              "    34,\n",
              "    34,\n",
              "    84,\n",
              "    68,\n",
              "    38,\n",
              "    15,\n",
              "    17,\n",
              "    66,\n",
              "    12,\n",
              "    37,\n",
              "    18,\n",
              "    23,\n",
              "    24,\n",
              "    22,\n",
              "    17,\n",
              "    27,\n",
              "    9,\n",
              "    14,\n",
              "    44,\n",
              "    31,\n",
              "    20,\n",
              "    14,\n",
              "    17,\n",
              "    15,\n",
              "    22,\n",
              "    24,\n",
              "    64,\n",
              "    60,\n",
              "    15,\n",
              "    11,\n",
              "    51,\n",
              "    29,\n",
              "    28,\n",
              "    30,\n",
              "    37,\n",
              "    25,\n",
              "    23,\n",
              "    35,\n",
              "    55,\n",
              "    41,\n",
              "    59,\n",
              "    15,\n",
              "    14,\n",
              "    22,\n",
              "    18,\n",
              "    69,\n",
              "    21,\n",
              "    31,\n",
              "    26,\n",
              "    25,\n",
              "    16,\n",
              "    17,\n",
              "    20,\n",
              "    45,\n",
              "    25,\n",
              "    18,\n",
              "    42,\n",
              "    31,\n",
              "    43,\n",
              "    18,\n",
              "    22,\n",
              "    31,\n",
              "    53,\n",
              "    31,\n",
              "    71,\n",
              "    39,\n",
              "    11,\n",
              "    30,\n",
              "    35,\n",
              "    18,\n",
              "    75,\n",
              "    55,\n",
              "    48,\n",
              "    49,\n",
              "    33,\n",
              "    30,\n",
              "    28,\n",
              "    20,\n",
              "    15,\n",
              "    19,\n",
              "    36,\n",
              "    47,\n",
              "    40,\n",
              "    21,\n",
              "    44,\n",
              "    17,\n",
              "    24,\n",
              "    17,\n",
              "    31,\n",
              "    29,\n",
              "    20,\n",
              "    31,\n",
              "    33,\n",
              "    22,\n",
              "    15,\n",
              "    14,\n",
              "    14,\n",
              "    20,\n",
              "    23,\n",
              "    51,\n",
              "    23,\n",
              "    20,\n",
              "    22,\n",
              "    13,\n",
              "    46,\n",
              "    19,\n",
              "    21],\n",
              "   'episode_reward': [40.0,\n",
              "    20.0,\n",
              "    10.0,\n",
              "    26.0,\n",
              "    34.0,\n",
              "    38.0,\n",
              "    45.0,\n",
              "    22.0,\n",
              "    26.0,\n",
              "    35.0,\n",
              "    20.0,\n",
              "    48.0,\n",
              "    18.0,\n",
              "    34.0,\n",
              "    49.0,\n",
              "    15.0,\n",
              "    27.0,\n",
              "    17.0,\n",
              "    17.0,\n",
              "    54.0,\n",
              "    115.0,\n",
              "    43.0,\n",
              "    34.0,\n",
              "    34.0,\n",
              "    84.0,\n",
              "    68.0,\n",
              "    38.0,\n",
              "    15.0,\n",
              "    17.0,\n",
              "    66.0,\n",
              "    12.0,\n",
              "    37.0,\n",
              "    18.0,\n",
              "    23.0,\n",
              "    24.0,\n",
              "    22.0,\n",
              "    17.0,\n",
              "    27.0,\n",
              "    9.0,\n",
              "    14.0,\n",
              "    44.0,\n",
              "    31.0,\n",
              "    20.0,\n",
              "    14.0,\n",
              "    17.0,\n",
              "    15.0,\n",
              "    22.0,\n",
              "    24.0,\n",
              "    64.0,\n",
              "    60.0,\n",
              "    15.0,\n",
              "    11.0,\n",
              "    51.0,\n",
              "    29.0,\n",
              "    28.0,\n",
              "    30.0,\n",
              "    37.0,\n",
              "    25.0,\n",
              "    23.0,\n",
              "    35.0,\n",
              "    55.0,\n",
              "    41.0,\n",
              "    59.0,\n",
              "    15.0,\n",
              "    14.0,\n",
              "    22.0,\n",
              "    18.0,\n",
              "    69.0,\n",
              "    21.0,\n",
              "    31.0,\n",
              "    26.0,\n",
              "    25.0,\n",
              "    16.0,\n",
              "    17.0,\n",
              "    20.0,\n",
              "    45.0,\n",
              "    25.0,\n",
              "    18.0,\n",
              "    42.0,\n",
              "    31.0,\n",
              "    43.0,\n",
              "    18.0,\n",
              "    22.0,\n",
              "    31.0,\n",
              "    53.0,\n",
              "    31.0,\n",
              "    71.0,\n",
              "    39.0,\n",
              "    11.0,\n",
              "    30.0,\n",
              "    35.0,\n",
              "    18.0,\n",
              "    75.0,\n",
              "    55.0,\n",
              "    48.0,\n",
              "    49.0,\n",
              "    33.0,\n",
              "    30.0,\n",
              "    28.0,\n",
              "    20.0,\n",
              "    15.0,\n",
              "    19.0,\n",
              "    36.0,\n",
              "    47.0,\n",
              "    40.0,\n",
              "    21.0,\n",
              "    44.0,\n",
              "    17.0,\n",
              "    24.0,\n",
              "    17.0,\n",
              "    31.0,\n",
              "    29.0,\n",
              "    20.0,\n",
              "    31.0,\n",
              "    33.0,\n",
              "    22.0,\n",
              "    15.0,\n",
              "    14.0,\n",
              "    14.0,\n",
              "    20.0,\n",
              "    23.0,\n",
              "    51.0,\n",
              "    23.0,\n",
              "    20.0,\n",
              "    22.0,\n",
              "    13.0,\n",
              "    46.0,\n",
              "    19.0,\n",
              "    21.0]},\n",
              "  'hostname': '78972b9d4096',\n",
              "  'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
              "     'learner_stats': {'cur_kl_coeff': 0.10000000149011612,\n",
              "      'cur_lr': 4.999999873689376e-05,\n",
              "      'entropy': 0.65860796,\n",
              "      'entropy_coeff': 0.0,\n",
              "      'kl': 0.008350546,\n",
              "      'model': {},\n",
              "      'policy_loss': -0.014851825,\n",
              "      'total_loss': 418.85956,\n",
              "      'vf_explained_var': 0.0009813446,\n",
              "      'vf_loss': 418.8736}}},\n",
              "   'num_agent_steps_sampled': 8000,\n",
              "   'num_agent_steps_trained': 8000,\n",
              "   'num_steps_sampled': 8000,\n",
              "   'num_steps_trained': 8000},\n",
              "  'iterations_since_restore': 2,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 1,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 61.330000000000005,\n",
              "   'ram_util_percent': 16.800000000000004},\n",
              "  'pid': 65,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.06839645279897569,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 0.08020873606733912,\n",
              "   'mean_inference_ms': 1.1911919647090925,\n",
              "   'mean_raw_obs_processing_ms': 0.11091091650424191},\n",
              "  'time_since_restore': 13.640817403793335,\n",
              "  'time_this_iter_s': 6.661553144454956,\n",
              "  'time_total_s': 13.640817403793335,\n",
              "  'timers': {'learn_throughput': 4381.158,\n",
              "   'learn_time_ms': 913.001,\n",
              "   'load_throughput': 10595021.156,\n",
              "   'load_time_ms': 0.378,\n",
              "   'sample_throughput': 620.133,\n",
              "   'sample_time_ms': 6450.231,\n",
              "   'update_time_ms': 3.325},\n",
              "  'timestamp': 1635530894,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 8000,\n",
              "  'training_iteration': 2},\n",
              " {'agent_timesteps_total': 12000,\n",
              "  'config': {'_fake_gpus': False,\n",
              "   '_tf_policy_handles_more_than_one_loss': False,\n",
              "   'action_space': None,\n",
              "   'actions_in_input_normalized': False,\n",
              "   'batch_mode': 'truncate_episodes',\n",
              "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
              "   'clip_actions': False,\n",
              "   'clip_param': 0.3,\n",
              "   'clip_rewards': None,\n",
              "   'collect_metrics_timeout': 180,\n",
              "   'compress_observations': False,\n",
              "   'create_env_on_driver': False,\n",
              "   'custom_eval_function': None,\n",
              "   'custom_resources_per_worker': {},\n",
              "   'eager_tracing': False,\n",
              "   'entropy_coeff': 0.0,\n",
              "   'entropy_coeff_schedule': None,\n",
              "   'env': 'CartPole-v1',\n",
              "   'env_config': {},\n",
              "   'env_task_fn': None,\n",
              "   'evaluation_config': {},\n",
              "   'evaluation_interval': None,\n",
              "   'evaluation_num_episodes': 10,\n",
              "   'evaluation_num_workers': 0,\n",
              "   'evaluation_parallel_to_training': False,\n",
              "   'exploration_config': {'type': 'StochasticSampling'},\n",
              "   'explore': True,\n",
              "   'extra_python_environs_for_driver': {},\n",
              "   'extra_python_environs_for_worker': {},\n",
              "   'fake_sampler': False,\n",
              "   'framework': 'tf',\n",
              "   'gamma': 0.99,\n",
              "   'grad_clip': None,\n",
              "   'horizon': None,\n",
              "   'ignore_worker_failures': False,\n",
              "   'in_evaluation': False,\n",
              "   'input': 'sampler',\n",
              "   'input_config': {},\n",
              "   'input_evaluation': ['is', 'wis'],\n",
              "   'kl_coeff': 0.2,\n",
              "   'kl_target': 0.01,\n",
              "   'lambda': 1.0,\n",
              "   'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
              "    'intra_op_parallelism_threads': 8},\n",
              "   'log_level': 'WARN',\n",
              "   'log_sys_usage': True,\n",
              "   'logger_config': None,\n",
              "   'lr': 5e-05,\n",
              "   'lr_schedule': None,\n",
              "   'metrics_smoothing_episodes': 100,\n",
              "   'min_iter_time_s': 0,\n",
              "   'model': {'_no_preprocessing': False,\n",
              "    '_time_major': False,\n",
              "    '_use_default_native_models': False,\n",
              "    'attention_dim': 64,\n",
              "    'attention_head_dim': 32,\n",
              "    'attention_init_gru_gate_bias': 2.0,\n",
              "    'attention_memory_inference': 50,\n",
              "    'attention_memory_training': 50,\n",
              "    'attention_num_heads': 1,\n",
              "    'attention_num_transformer_units': 1,\n",
              "    'attention_position_wise_mlp_dim': 32,\n",
              "    'attention_use_n_prev_actions': 0,\n",
              "    'attention_use_n_prev_rewards': 0,\n",
              "    'conv_activation': 'relu',\n",
              "    'conv_filters': None,\n",
              "    'custom_action_dist': None,\n",
              "    'custom_model': None,\n",
              "    'custom_model_config': {},\n",
              "    'custom_preprocessor': None,\n",
              "    'dim': 84,\n",
              "    'fcnet_activation': 'tanh',\n",
              "    'fcnet_hiddens': [100, 50],\n",
              "    'framestack': True,\n",
              "    'free_log_std': False,\n",
              "    'grayscale': False,\n",
              "    'lstm_cell_size': 256,\n",
              "    'lstm_use_prev_action': False,\n",
              "    'lstm_use_prev_action_reward': -1,\n",
              "    'lstm_use_prev_reward': False,\n",
              "    'max_seq_len': 20,\n",
              "    'no_final_linear': False,\n",
              "    'post_fcnet_activation': 'relu',\n",
              "    'post_fcnet_hiddens': [],\n",
              "    'use_attention': False,\n",
              "    'use_lstm': False,\n",
              "    'vf_share_layers': False,\n",
              "    'zero_mean': True},\n",
              "   'monitor': -1,\n",
              "   'multiagent': {'count_steps_by': 'env_steps',\n",
              "    'observation_fn': None,\n",
              "    'policies': {'default_policy': PolicySpec(policy_class=<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, observation_space=None, action_space=None, config={})},\n",
              "    'policies_to_train': None,\n",
              "    'policy_map_cache': None,\n",
              "    'policy_map_capacity': 100,\n",
              "    'policy_mapping_fn': None,\n",
              "    'replay_mode': 'independent'},\n",
              "   'no_done_at_end': False,\n",
              "   'normalize_actions': True,\n",
              "   'num_cpus_for_driver': 1,\n",
              "   'num_cpus_per_worker': 0,\n",
              "   'num_envs_per_worker': 1,\n",
              "   'num_gpus': 0,\n",
              "   'num_gpus_per_worker': 0,\n",
              "   'num_sgd_iter': 10,\n",
              "   'num_workers': 1,\n",
              "   'observation_filter': 'NoFilter',\n",
              "   'observation_space': None,\n",
              "   'optimizer': {},\n",
              "   'output': None,\n",
              "   'output_compress_columns': ['obs', 'new_obs'],\n",
              "   'output_max_file_size': 67108864,\n",
              "   'placement_strategy': 'PACK',\n",
              "   'postprocess_inputs': False,\n",
              "   'preprocessor_pref': 'deepmind',\n",
              "   'record_env': False,\n",
              "   'remote_env_batch_wait_ms': 0,\n",
              "   'remote_worker_envs': False,\n",
              "   'render_env': False,\n",
              "   'rollout_fragment_length': 200,\n",
              "   'sample_async': False,\n",
              "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
              "   'seed': None,\n",
              "   'sgd_minibatch_size': 250,\n",
              "   'shuffle_buffer_size': 0,\n",
              "   'shuffle_sequences': True,\n",
              "   'simple_optimizer': False,\n",
              "   'soft_horizon': False,\n",
              "   'synchronize_filters': True,\n",
              "   'tf_session_args': {'allow_soft_placement': True,\n",
              "    'device_count': {'CPU': 1},\n",
              "    'gpu_options': {'allow_growth': True},\n",
              "    'inter_op_parallelism_threads': 2,\n",
              "    'intra_op_parallelism_threads': 2,\n",
              "    'log_device_placement': False},\n",
              "   'timesteps_per_iteration': 0,\n",
              "   'train_batch_size': 4000,\n",
              "   'use_critic': True,\n",
              "   'use_gae': True,\n",
              "   'vf_clip_param': 10.0,\n",
              "   'vf_loss_coeff': 1.0,\n",
              "   'vf_share_layers': -1},\n",
              "  'custom_metrics': {},\n",
              "  'date': '2021-10-29_18-08-20',\n",
              "  'done': False,\n",
              "  'episode_len_mean': 40.3,\n",
              "  'episode_media': {},\n",
              "  'episode_reward_max': 147.0,\n",
              "  'episode_reward_mean': 40.3,\n",
              "  'episode_reward_min': 12.0,\n",
              "  'episodes_this_iter': 98,\n",
              "  'episodes_total': 396,\n",
              "  'experiment_id': '2821ac43e99b4fb980c141f7206d3200',\n",
              "  'hist_stats': {'episode_lengths': [19,\n",
              "    21,\n",
              "    20,\n",
              "    31,\n",
              "    35,\n",
              "    14,\n",
              "    35,\n",
              "    79,\n",
              "    36,\n",
              "    17,\n",
              "    33,\n",
              "    12,\n",
              "    39,\n",
              "    19,\n",
              "    14,\n",
              "    38,\n",
              "    27,\n",
              "    48,\n",
              "    38,\n",
              "    39,\n",
              "    34,\n",
              "    24,\n",
              "    18,\n",
              "    27,\n",
              "    20,\n",
              "    46,\n",
              "    29,\n",
              "    41,\n",
              "    147,\n",
              "    28,\n",
              "    34,\n",
              "    41,\n",
              "    76,\n",
              "    21,\n",
              "    46,\n",
              "    36,\n",
              "    20,\n",
              "    25,\n",
              "    26,\n",
              "    26,\n",
              "    134,\n",
              "    38,\n",
              "    79,\n",
              "    19,\n",
              "    46,\n",
              "    28,\n",
              "    53,\n",
              "    47,\n",
              "    19,\n",
              "    28,\n",
              "    50,\n",
              "    28,\n",
              "    23,\n",
              "    74,\n",
              "    58,\n",
              "    37,\n",
              "    23,\n",
              "    25,\n",
              "    40,\n",
              "    28,\n",
              "    78,\n",
              "    14,\n",
              "    32,\n",
              "    29,\n",
              "    37,\n",
              "    70,\n",
              "    18,\n",
              "    24,\n",
              "    14,\n",
              "    87,\n",
              "    33,\n",
              "    72,\n",
              "    54,\n",
              "    27,\n",
              "    15,\n",
              "    22,\n",
              "    32,\n",
              "    22,\n",
              "    84,\n",
              "    58,\n",
              "    27,\n",
              "    25,\n",
              "    41,\n",
              "    69,\n",
              "    22,\n",
              "    44,\n",
              "    32,\n",
              "    39,\n",
              "    26,\n",
              "    95,\n",
              "    63,\n",
              "    49,\n",
              "    46,\n",
              "    65,\n",
              "    30,\n",
              "    47,\n",
              "    56,\n",
              "    73,\n",
              "    13,\n",
              "    90],\n",
              "   'episode_reward': [19.0,\n",
              "    21.0,\n",
              "    20.0,\n",
              "    31.0,\n",
              "    35.0,\n",
              "    14.0,\n",
              "    35.0,\n",
              "    79.0,\n",
              "    36.0,\n",
              "    17.0,\n",
              "    33.0,\n",
              "    12.0,\n",
              "    39.0,\n",
              "    19.0,\n",
              "    14.0,\n",
              "    38.0,\n",
              "    27.0,\n",
              "    48.0,\n",
              "    38.0,\n",
              "    39.0,\n",
              "    34.0,\n",
              "    24.0,\n",
              "    18.0,\n",
              "    27.0,\n",
              "    20.0,\n",
              "    46.0,\n",
              "    29.0,\n",
              "    41.0,\n",
              "    147.0,\n",
              "    28.0,\n",
              "    34.0,\n",
              "    41.0,\n",
              "    76.0,\n",
              "    21.0,\n",
              "    46.0,\n",
              "    36.0,\n",
              "    20.0,\n",
              "    25.0,\n",
              "    26.0,\n",
              "    26.0,\n",
              "    134.0,\n",
              "    38.0,\n",
              "    79.0,\n",
              "    19.0,\n",
              "    46.0,\n",
              "    28.0,\n",
              "    53.0,\n",
              "    47.0,\n",
              "    19.0,\n",
              "    28.0,\n",
              "    50.0,\n",
              "    28.0,\n",
              "    23.0,\n",
              "    74.0,\n",
              "    58.0,\n",
              "    37.0,\n",
              "    23.0,\n",
              "    25.0,\n",
              "    40.0,\n",
              "    28.0,\n",
              "    78.0,\n",
              "    14.0,\n",
              "    32.0,\n",
              "    29.0,\n",
              "    37.0,\n",
              "    70.0,\n",
              "    18.0,\n",
              "    24.0,\n",
              "    14.0,\n",
              "    87.0,\n",
              "    33.0,\n",
              "    72.0,\n",
              "    54.0,\n",
              "    27.0,\n",
              "    15.0,\n",
              "    22.0,\n",
              "    32.0,\n",
              "    22.0,\n",
              "    84.0,\n",
              "    58.0,\n",
              "    27.0,\n",
              "    25.0,\n",
              "    41.0,\n",
              "    69.0,\n",
              "    22.0,\n",
              "    44.0,\n",
              "    32.0,\n",
              "    39.0,\n",
              "    26.0,\n",
              "    95.0,\n",
              "    63.0,\n",
              "    49.0,\n",
              "    46.0,\n",
              "    65.0,\n",
              "    30.0,\n",
              "    47.0,\n",
              "    56.0,\n",
              "    73.0,\n",
              "    13.0,\n",
              "    90.0]},\n",
              "  'hostname': '78972b9d4096',\n",
              "  'info': {'learner': {'default_policy': {'custom_metrics': {},\n",
              "     'learner_stats': {'cur_kl_coeff': 0.10000000149011612,\n",
              "      'cur_lr': 4.999999873689376e-05,\n",
              "      'entropy': 0.62853587,\n",
              "      'entropy_coeff': 0.0,\n",
              "      'kl': 0.004564244,\n",
              "      'model': {},\n",
              "      'policy_loss': -0.009513464,\n",
              "      'total_loss': 646.84265,\n",
              "      'vf_explained_var': 0.00026360527,\n",
              "      'vf_loss': 646.8517}}},\n",
              "   'num_agent_steps_sampled': 12000,\n",
              "   'num_agent_steps_trained': 12000,\n",
              "   'num_steps_sampled': 12000,\n",
              "   'num_steps_trained': 12000},\n",
              "  'iterations_since_restore': 3,\n",
              "  'node_ip': '172.28.0.2',\n",
              "  'num_healthy_workers': 1,\n",
              "  'off_policy_estimator': {},\n",
              "  'perf': {'cpu_util_percent': 61.45, 'ram_util_percent': 16.800000000000004},\n",
              "  'pid': 65,\n",
              "  'policy_reward_max': {},\n",
              "  'policy_reward_mean': {},\n",
              "  'policy_reward_min': {},\n",
              "  'sampler_perf': {'mean_action_processing_ms': 0.06859939882183012,\n",
              "   'mean_env_render_ms': 0.0,\n",
              "   'mean_env_wait_ms': 0.07999250517124663,\n",
              "   'mean_inference_ms': 1.1900904758084765,\n",
              "   'mean_raw_obs_processing_ms': 0.10849615975582882},\n",
              "  'time_since_restore': 20.28170680999756,\n",
              "  'time_this_iter_s': 6.640889406204224,\n",
              "  'time_total_s': 20.28170680999756,\n",
              "  'timers': {'learn_throughput': 4588.341,\n",
              "   'learn_time_ms': 871.775,\n",
              "   'load_throughput': 10296981.997,\n",
              "   'load_time_ms': 0.388,\n",
              "   'sample_throughput': 612.879,\n",
              "   'sample_time_ms': 6526.573,\n",
              "   'update_time_ms': 3.341},\n",
              "  'timestamp': 1635530900,\n",
              "  'timesteps_since_restore': 0,\n",
              "  'timesteps_total': 12000,\n",
              "  'training_iteration': 3}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "7fT1ZzX7emgi",
        "outputId": "e5c59376-86a2-40d9-fab9-fa5951a05ece"
      },
      "source": [
        "# Convert to df and inspect\n",
        "df = pd.DataFrame(data=episode_data)\n",
        "df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_len_mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>95.91</td>\n",
              "      <td>448.0</td>\n",
              "      <td>95.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>133.99</td>\n",
              "      <td>500.0</td>\n",
              "      <td>133.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10.0</td>\n",
              "      <td>167.49</td>\n",
              "      <td>500.0</td>\n",
              "      <td>167.49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   n  episode_reward_min  ...  episode_reward_max  episode_len_mean\n",
              "0  0                10.0  ...               448.0             95.91\n",
              "1  1                10.0  ...               500.0            133.99\n",
              "2  2                10.0  ...               500.0            167.49\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O12lx7-ogbiV"
      },
      "source": [
        "Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "T4g0uNPfgdhz",
        "outputId": "69b14435-a5a6-423e-fdf7-b28f0e4ffc19"
      },
      "source": [
        "df.plot(x=\"n\", y=[\"episode_reward_mean\", \"episode_reward_min\", \"episode_reward_max\"], secondary_y=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6d6a8652d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1f3/8dfJvg0kZAEkYVGpsipL2WIsFgUUBNvSVnGBatVqESy/1tKvS9XWR61QUKt8LSpKlQqKVtBqq2X5qihiQGUX2WSRJYQtC0lIcn5/zJ1hJnsgyYTL+/l4zCN37npyM7w587l3zhhrLSIi4l5hoW6AiIg0LgW9iIjLKehFRFxOQS8i4nIKehERl4uoz8phYWE2Nja2sdoiIuJKhYWF1lobso51vYI+NjaWgoKCxmqLiIgrGWOOh/L4Kt2IiLicgl5ExOUU9CIiLlevGr1IQzlx4gS7d++mqKgo1E0RaTAxMTGkp6cTGRkZ6qYEUdBLSOzevRuPx0PHjh0xxoS6OSKnzVpLbm4uu3fvplOnTqFuThCVbiQkioqKSE5OVsiLaxhjSE5OPqV3qcaYHcaYtcaYL4wx2c68VsaY940xXzs/k5z5xhjzpDFmizFmjTGmd237V9BLyCjkxW1O8zV9mbX2YmttX+f5FGCxtbYzsNh5DnAl0Nl53Ab8b207VulGzkjFZcXsydvDrrxd7MrbxZHiI6Fukrjc7RfdTmRYk9beRwODnek5wDLgt878v1vvGPMrjDGJxpi21tq91e1IQS/NVl5Jnj/IKz72F+zHEvxdCga9Q5DGc2vPW09n8whfScYxy1o7K+C5Bd4zxljgb86y1gHhvQ9o7Uy3A3YFbLvbmaegl+bHWktuUW5QgO88tpPdebvZlbeLw8WHg9ZvFdOK9p72fLf1d8nwZJDRIsP705NBUnRSo5eCHnjgAS699FIuv/zy09pPQkIC+fn5DdSqprFs2TKmTZvG22+/HeqmVOn48eMMHz6cJUuWEB4eXmn5oEGD+Pjjj2vcR8eOHcnOziYlJSVo/rJly4iKiiI6PBqAp556iri4OG6++eb6NLE0oCRTlUustXuMMWnA+8aYTYELrbXW+U/glCjopVGVlZexr3AfO4/tZFfeLnbn7WZn3k625GzhwOoDHC89+cnwMBNGm7g2ZLTIYEiHIWR4MmjvaU+GJ4N0TzrxkfEh/E3g4YcfDunxfay1WGsJC2u8S2xlZWVVBmZzNXv2bH74wx9WanNpaSkRERG1hnxNli1bRkJCAoMGDQLg5ptvJjMzs75BXyNr7R7n5wFjzD+BfsB+X0nGGNMWOOCsvgfICNg83ZlXLQW9nDZfvXxn3s6TPfM8b898T/4eSstL/etGhUXRztOO5Ohksjpmke5J58MNln25CUTZZMzxCPJyYQPeh9ch51F3Xc9pwe+v7lbjOi+//DJPPvkkJSUl9O/fn5kzZxIeHk5CQgK33nor7733Hm3atGHevHmkpqYyfvx4Ro4cyZgxY5gyZQqLFi0iIiKCoUOHMm3aNHbs2MHNN9/MwYMHSU1N5YUXXqB9+/Zs376dsWPHkp+fz+jRo4PaMHXqVF599VWKi4v5wQ9+wEMPPVRlW3fs2MGwYcPo378/q1at4p133uHVV1+ttO3UqVOJjo5m4sSJ/OpXv+LLL79kyZIlLFmyhOeff565c+dyxx138Nlnn3H8+HHGjBnjP2bHjh356U9/yvvvv88999xDYmIid999N3FxcVxyySU1nssHH3yQ7du3s23bNnbu3MmMGTNYsWIF7777Lu3ateOtt94iMjKSVatWMXnyZPLz80lJSeHFF1+kbdu2PPvss8yaNYuSkhLOP/98XnrpJeLi4hg/fjwtWrQgOzubffv28dhjjzFmzJhKx587dy7/+Mc/AG8w33///SQlJbFp0yY2b97sfxdVXl7OhAkTWLJkCRkZGURGRnLzzTf79/nXv/6Vt956ixMnTvDaa68RExPDM888Q3h4OC+//DJ//etfycrKomPHjqxcuZJ+/frVeF7qwhgTD4RZa/Oc6aHAw8AiYBzwqPNzobPJImCCMWYe0B84WlN9HhT0Uke+erkvwH1lll15uzhQeCCoXp4QmUCGJ4MLki7g8vaX+8sr7Vu0Jy0ujTATxsaNG+nSpQsAW7as57A91qS/z8aNG5k/fz7Lly8nMjKSO++8k7lz53LTTTdRUFBA3759mTFjBg8//DAPPfQQTz31lH/b3Nxc/vnPf7Jp0yaMMRw54r0QfNdddzFu3DjGjRvH7NmzmThxIm+++SaTJk3ijjvu4KabbuLpp5/27+e9997j66+/ZuXKlVhrGTVqFB988AGXXnpplW3++uuvmTNnDgMGDKh226ysLP7yl78wceJEsrOzKS4u5sSJE3z44Yf+/T7yyCO0atWKsrIyhgwZwpo1a+jZsycAycnJrF69mqKiIjp37sySJUs4//zz+elPf1rrOd26dStLly5lw4YNDBw4kNdff53HHnuMH/zgB/zrX/9ixIgR3HXXXSxcuJDU1FTmz5/Pvffe6++N33qrtwZ+33338fzzz3PXXXcBsHfvXj766CM2bdrEqFGjKgV9SUkJ27Zto2PHjv55q1evZt26dZXuZ3/jjTfYsWMHGzZs4MCBA3Tp0iWoZ56SksLq1auZOXMm06ZN47nnnuMXv/gFCQkJ/PrXv/av17dvXz788MMGCXq8tfd/OqXHCOAf1tp/G2M+A141xtwCfAP8xFn/HeAqYAtQCPystgMo6AUIrpf7Ajyw1FLxrpbkmGQyPBn0b9ufdE/6yTD3tCcxOrFe9fLaet6NYfHixaxatYrvfve7gLfGm5aWBkBYWJg/2G644QZ++MMfBm3bsmVLYmJiuOWWWxg5ciQjR44E4JNPPuGNN94A4MYbb+See+4BYPny5bz++uv++b/97W8Bb9C/99579OrVC4D8/Hy+/vrraoO+Q4cODBgwoMZtb7rpJlatWsWxY8eIjo6md+/eZGdn8+GHH/Lkk08C8OqrrzJr1ixKS0vZu3cvGzZs8Ae97/fetGkTnTp1onPnzv7zMGvWrIpNCnLllVcSGRlJjx49KCsrY/jw4QD06NGDHTt28NVXX7Fu3TquuOIKwFseatu2LQDr1q3jvvvu48iRI+Tn5zNs2DD/fq+55hrCwsLo2rUr+/fvr3TcgwcPkpiYGDSvX79+VX5o6aOPPuLHP/4xYWFhtGnThssuuyxoue9v3adPH//fsippaWls2rSp2uX1Ya3dBlxUxfxcYEgV8y3wy/ocQ0F/FiktL2Vfwb5q72SpWC9vG9+WdE86V3S4wh/kvkdcZFwIf5PTZ61l3Lhx/OlPf6p13Yr/aUVERLBy5UoWL17MggULeOqpp1iyZEm99uFrw+9+9ztuv/32OrU5Pv7kNYqatu3UqRMvvvgigwYNomfPnixdupQtW7bQpUsXtm/fzrRp0/jss89ISkpi/PjxQR/wCTxGfUVHey9WhoWFERkZ6f+dw8LCKC0txVpLt27d+OSTTyptO378eN58800uuugiXnzxRZYtW1Zpv77fu6LY2NhKH1I61d/Dd6zw8HBKS0urXa+oqIgz6bs5FPQuU1xW7C+t+Hvn+d6e+Z68PZTa4Hq5rzfer02/oCBvl9COyPDmNV5HQxoyZAijR4/mV7/6FWlpaRw6dIi8vDw6dOhAeXk5CxYs4Nprr+Uf//hHpfp0fn4+hYWFXHXVVWRmZnLuuecC3js75s2bx4033sjcuXPJysoCIDMzk3nz5nHDDTcwd+5c/36GDRvG/fffz/XXX09CQgJ79uwhMjLS/86iJjVtm5WVxbRp05g9ezY9evRg8uTJ9OnTB2MMx44dIz4+npYtW7J//37effddBg8eXGn/F154ITt27GDr1q2cd955vPLKK6dxtr0uuOACcnJy+OSTTxg4cCAnTpxg8+bNdOvWjby8PNq2bcuJEyeYO3cu7dq1q/N+k5KSKCsro6ioiJiYmBrXzczMZM6cOYwbN46cnByWLVvG2LFja9zG4/Fw7FhwaXHz5s1kZmbWuY2hpqA/Ax0rORZcWgkotewvDH5rW7Fe3r5Fe3+Y++rlZ6OuXbvyxz/+kaFDh1JeXk5kZCRPP/00HTp0ID4+npUrV/LHP/6RtLQ05s+fH7RtXl4eo0ePpqioCGst06dPB7wX8n72s58xdepU/8VYgCeeeIKxY8fy5z//Oehi7NChQ9m4cSMDBw4EvLddvvzyy3UK+pq2zcrK4pFHHmHgwIHEx8cTExPj/0/noosuolevXlx44YVkZGRUG1YxMTHMmjWLESNGEBcXR1ZWFnl5efU8y8GioqJYsGABEydO5OjRo5SWlnL33XfTrVs3/vCHP9C/f39SU1Pp379/vY81dOhQPvroo1pvff3Rj37E4sWL6dq1KxkZGfTu3ZuWLVvWuM3VV1/NmDFjWLhwof9i7PLly3nwwQfr1cZQMlW9FapOfHy81TdMNT5fvTwwwAMvglasl6fEpvjDO92T7r8lMcOTUe96eVMJvBjb3JyJ97mf7VavXs2MGTN46aWXal03Pz+fhIQEcnNz6devH8uXL6dNmzZ1Ptbnn3/O9OnTqz1WVa9tY0yhtTZk9werRx8ivnp5pbtYnDJLVfXyDE8GV3S4IujecjfUy0VOV+/evbnsssvqdP//yJEjOXLkCCUlJdx///31CnnwXvz9wx/+cDrNbXLq0TeiotIi9uTvCeqZ+x7f5n9bqV4e2Cv33Y6Y4cngnPhzXFcvb849+lDKzc1lyJBKN1qwePFikpOTQ9CiYC+88AJPPPFE0LzMzMyg20bPds2xR6+gP03+evmx4CDfmbeTA4UHgtb1RHqCPrYf+Djb6uUKenGr5hj0Kt3UwlrLweMH/eFd8f7yo8VHg9ZPiU2hvac9A9oOCLq3PMOTQcvols2yXi4i7qagx1sv31uwt3LPvJZ6+bAOw072yltkkJ6Qrnq5iDQ7Z03QF5UWBd9fHnARtGK9PDo8mvQEb528Ys+8bULbph6TWkTktLgq6I8WH60U5r7pSvXyKA8Zngy6JHdhWMdhQbcmpsalnlX1chFxtzMq6K215BzPCb6D5djJMkvFenlqbKq/Vx54b3n7Fu1pGV3zhyREKtJ49Kc/Hv3Pf/5zJk+eTNeuXeu8zZtvvsmaNWt44IEHKi1btGgRGzZsYMqUKVVs6VVT2x9//HFuu+024uK8JdfLL7+c1157jaSkpDq370zQ7ILeXy8/tqtSz3xP/p6genm4CQ+ql7dv0d5/a6Lq5dLQNB796Xvuuefqvc1jjz3GokWLKs0vLS1l1KhRjBo16pTb8/jjj3PDDTf4g/7GG29k5syZ3Hvvvae8z+YoJEHvq5cHllZ8j735eyvVy31llYHnDAzqmate7hLvToF9axt2n216wJWP1riKxqNv+vHoBw8ezLRp0+jbty8JCQlMmjSJt99+m9jYWBYuXEjr1q2D9rt582aio6P93/o0fvx4YmJi+Pzzz8nMzKRnz55kZ2fz1FNPsXXrVq6//noKCgoYPXo0jz/+uP+dU35+PmPGjGHdunX06dPHP7b8t99+y2WXXUZKSgpLly5l1KhRZGVlKehPxUsbXmLz4c3+r4k7cLxyvby9pz3dkrsxvOPwoPvLVS+XxqDx6Jt+PPprrrkmaP2CggIGDBjAI488wj333MOzzz7LfffdF7TO8uXL6d27d9C83bt38/HHHxMeHs6LL77onz9p0iQmTZrEddddxzPPPBO0zeeff8769es555xzyMzMZPny5UycOJHp06ezdOlS/38kSUlJFBcXk5ub2yw+oNZQmiTo39r6FgePHyTDk8HAcwYGferTd3+5nMVq6Xk3Bo1H3/Tj0VcUFRXlP3d9+vTh/fffr7TO3r17SU1NDZr34x//uMqy0ieffMKbb74JwNixY4O+KKRfv36kp6cDcPHFF7Njx45q36WkpaXx7bffKujra+6IuSqxSLOi8eibfjz6igLXqW7899jYWI4eDb7J4lTaGDimvdvGmq+LJqmJKOSluRkyZAgLFizgwAFvGfHQoUN88803AP7x6IFqx6M/evQoV111FTNmzODLL78ETo5HD1Q5Hr1vvs+wYcOYPXu2v468Z88ef3tqU9O2vvHoL730UrKysnjmmWfo1atXtePRVyVwPHqgQcajPxVdunRhy5YtdVp3wIAB/ndOvvNdG4/HEzQksrWWffv2BX0toRuo+C1npcDx6Hv27MkVV1zB3r3e71f2jUffvXt3lixZUum2vry8PEaOHEnPnj255JJLgsajf+GFF+jZsycvvfSSf/CvJ554gqeffpoePXqwZ88e/36GDh3K2LFjGThwID169GDMmDF1Hoe9pm2zsrLYu3cvAwcOpHXr1tWORz927Ng6jUffu3fvOo2R3xguvfRSPv/88yq/Waqixx9/nOnTp9OzZ0+2bNlS6zjzALfddhvDhw/3f6XgqlWrGDBgABERze6GxNOiQc0kJJrzoGZn4n3ubjZp0iSuvvrqWj+/UFhYSGxsLMYY5s2bxyuvvMLChQvrfaxRo0ZVOYJoXWlQMxGRevqf//kfPv3001rXW7VqFRMmTMBaS2JiIrNnz673sbp3735aId9cqUcvIdGce/ShpPHoz3zNsUevoJeQUNCLWzXHoNfFWBERl1PQi4i4nIJeRMTlFPQiIi6noBepowceeID//ve/p72fhISEBmhN01q2bJl/XJpQuvvuu/nggw+qXFaXv8+DDz7ItGnTKs0/cuQIM2fO9D/Pycnxj9XjBgp6kTp6+OGHT/tLRxqCtZby8vJGPUZZWVmj7v9U5ObmsmLFiioHfSsrKzutv0/FoE9NTaVt27YsX778lNvbnOgDUxJyf175ZzYd2tSg+7yw1YX8tt9va1xH49E3/Xj0Dz/8MG+99RbHjx9n0KBB/O1vf6OsrIyBAwcydepUBg8ezO9+9zvCwsJ45JFHgo7x+uuvB/WyK7b33//+t//v88477zB58mTi4+PJzMxk27Zt/m+Y2rBhA4MHD2bnzp3cfffdTJw4kSlTprB161YuvvhirrjiCqZOnco111zD3Llzqx0m4kyiHr2clQLHo//iiy8IDw/3DzjmG49+/fr1fO9736sUvr7x6NevX8+aNWv8Y6j7xqNfs2YN119/PRMnTgTwj0e/du1a2rZt699P4JjyX3zxBatWraq2LAHe8ejvvPNO1q9fz1dffVXltllZWXz44YcAZGdnk5+fX+V49NnZ2axZs4b/+7//Y82aNf5j+Majv+aaa7j11lt56623WLVqFfv27av1nG7dupUlS5awaNEibrjhBi677DLWrl1LbGws//rXvwCYMGECn332GevWreP48eO8/fbbRERE8OKLL3LHHXfw3//+l3//+9/8/ve/r7T/5cuX06dPn6B5vvZee+21/nlFRUXcfvvtvPvuu6xatYqcnJygbTZt2sR//vMfVq5cyUMPPcSJEyd49NFHOe+88/jiiy+YOnUqAH379vWfyzOdevQScrX1vBuDxqMPzXj0S5cu5bHHHqOwsJBDhw7RrVs3rr76arp168aNN97IyJEj+eSTT4iKiqq0/6rGpq/qC1E2bdrEueeeS6dOnQC47rrrgto+YsQIoqOjiY6OJi0tjf3791f5+/jGpW8qxphwIBvYY60daYzpBMwDkoFVwI3W2hJjTDTwd6APkAv81Fq7o6Z9K+jlrKTx6Jt+PPqioiLuvPNOsrOzycjI4MEHHww69tq1a0lMTKx2qObY2Nig9U+1vXUdmz4E49JPAjYCLZznfwZmWGvnGWOeAW4B/tf5edhae74x5lpnvRq/AkylGzkraTz6ph+P3hfSKSkp5Ofn+88xwBtvvMGhQ4f44IMPuOuuu/xfzxiormPTX3DBBWzbts3/LmL+/Pm1blNxXHrwfl9t9+7da922IRhj0oERwHPOcwN8H/CdpDmA77sYRzvPcZYPMVX1JAIo6OWspPHom348+sTERG699Va6d+/OsGHD/GWzgwcPMmXKFJ577jm+853vMGHCBCZNmlRp+xEjRrBs2bJajxMbG8vMmTMZPnw4ffr0wePx1Do2fXJyMpmZmXTv3p3f/OY3gLfMNGLEiPr/oqfmceAewHc7VTJwxFrre7uxG2jnTLcDdgE4y48661fPWlvnR1xcnBVpCBs2bAh1E6oVHx8f6iZINTIzM+3hw4drXS8vL89aa215ebm944477PTp0+t9rKysLHvo0KF6b1fVaxsoxlt/9z1us06uAiOBmc70YOBtIAXYErBOBrDOmV4HpAcs2wqk2BqyWzV6ETlj/OUvf2Hnzp0kJibWuN6zzz7LnDlzKCkpoVevXnW+DuKTk5PD5MmTSUpKOp3mBiq11vatZlkmMMoYcxUQg7dG/wSQaIyJsN5eezrgezu4B2/w7zbGRAAt8V6UrZaGKZaQ0DDFVdN49Ge+0xmm2BgzGPi19d518xrwuj15MXaNtXamMeaXQA9r7S+ci7E/tNb+pMb9KuglFDZu3MiFF15Y5d0oImcqay2bNm1qqKA/F+/tla2Az4EbrLXFxpgY4CWgF3AIuNZau63G/SroJRS2b9+Ox+MhOTlZYS+uYK0lNzeXvLw8/z38PqH+4hHV6CUk0tPT2b17d6VPLYqcyWJiYkhPTw91MypRj15EpJGFukev++hFRFxOQS8i4nIKehERl1PQi4i4nIJeRMTlFPQiIi6noBcRcTkFvYiIyynoRURcTkEvIuJyCnoREZdT0IuIuJyCXkTE5RT0IiIup6AXEXE5Bb2IiMsp6EVEXE5BLyLicgp6ERGXU9CLiLicgl5ExOUU9CIiLqegFxFxOQW9iIjLKehFRFxOQS8i4nIKehERl1PQi4i4nIJeRMTlFPQiIi6noBcRCSFjTIwxZqUx5ktjzHpjzEPO/E7GmE+NMVuMMfONMVHO/Gjn+RZnecfajqGgFxEJrWLg+9bai4CLgeHGmAHAn4EZ1trzgcPALc76twCHnfkznPVqpKAXEQkh65XvPI10Hhb4PrDAmT8HuMaZHu08x1k+xBhjajqGgl5EpPFFGGOyAx63BS40xoQbY74ADgDvA1uBI9baUmeV3UA7Z7odsAvAWX4USK7x4A33e4iISDVKrbV9q1torS0DLjbGJAL/BC5syIOrRy8i0kxYa48AS4GBQKIxxtcZTwf2ONN7gAwAZ3lLILem/SroRURCyBiT6vTkMcbEAlcAG/EG/hhntXHAQmd6kfMcZ/kSa62t6Rgq3YiIhFZbYI4xJhxv5/tVa+3bxpgNwDxjzB+Bz4HnnfWfB14yxmwBDgHX1nYAU8t/BEHi4+NtQUFBPX8HEZGzmzGm0FobH6rjq3QjIuJyCnoREZdT0IuIuJyCXkTE5RT0IiIup6AXEXE5Bb2IiMsp6EVEXE5BLyLicgp6ERGXU9CLiLicgl5ExOUU9CIiLqegFxFxOQW9iIjLKehFRFxOQS8i4nIKehERl1PQi4i4nIJeRMTlFPQiIi6noBcRcTkFvYiIyynoRURcTkEvIuJyCnoREZdT0IuIuJyCXkTE5RT0IiIup6AXEXE5Bb2IiMsp6EVEQsgYk2GMWWqM2WCMWW+MmeTMb2WMed8Y87XzM8mZb4wxTxpjthhj1hhjetd2DAW9iEholQL/z1rbFRgA/NIY0xWYAiy21nYGFjvPAa4EOjuP24D/re0ACnoRkRCy1u611q52pvOAjUA7YDQwx1ltDnCNMz0a+Lv1WgEkGmPa1nQMBb2ISOOLMMZkBzxuq2olY0xHoBfwKdDaWrvXWbQPaO1MtwN2BWy225lX/cFPo+EiIlI3pdbavjWtYIxJAF4H7rbWHjPG+JdZa60xxp7qwdWjFxEJMWNMJN6Qn2utfcOZvd9XknF+HnDm7wEyAjZPd+ZVS0EvIhJCxtt1fx7YaK2dHrBoETDOmR4HLAyYf5Nz980A4GhAiafqY1hb93cD8fHxtqCgoM7ri4gIGGMKrbXx1Sy7BPgQWAuUO7P/B2+d/lWgPfAN8BNr7SHnP4angOFAIfAza212jcdX0IuINK6agr4pqHQjIuJyCnoREZdT0IuIuJyCXkTE5RT0IiIup6AXEXE5DYEgIlJB0YkycvKKOZhfzMH8Eg4XlPCT72bUvmEzpaAXEdez1pJXXMrBPG9wH8wvJje/mBxn+mBeMbkFJ6cLSsoq7WPkRW2JizozI/PMbLWInPXKyy2HC0v8we3rfVcV3AcLSigpLa+0D2OgVVwUKQnRpHiiuCg90T+dEu/8TIgmJSGamIjwEPyWDUNBLyLNRklpOYecgM7JLyY3ILgP5nvDO8fplR8qKKa8ig/2R4Ybkp2QTo6PpnOahxRPFKkJ0SQnnAzulIRokuIiiQh3/6VKBb2INKrjJWX+4Pb3tPMq9MCd6aPHT1S5j9jIcH/vOj0pjl7tvT3v5PgoUjzRAeEdRcvYSAKH+BUFvYjUk7WWY0WlAT3tEnILvNM5+SXkViijFFZR7wZoERPhDen4aC5o4yHTCevAXrevFx4frag6HTp7IkKZv95dzME8b3DnVLhweXK6hJKyyvXuMAOt4qP8Ye3rdfuep/p63Z4oWsVHEX0G17zPNAp6EZcqKS13etpVXKwMrH/nF3OooKTaendgL/uCNh5/iSQlILiT46NpFR9FeJhKJs2Rgl7kDFJYUsrBvBJvvbtCWOfmn5x/MK+YY0WlVe4jLircH94ZreLo1T6J1IQokgPq3L6SSovYCNW7XUBBLxJC1lqOHS+tMrgrXazMK+H4iarr3S1jI0lxwrpLmxaknO8roQQHd4on6oy9F1xOnf7iIg2srNz6bxEMDO+cgPp3YC38RFnlmom33n2yRNKhfVyl4PZdqEyOjyYqwv23CMqpU9CL1EFxaVmF3nbwhUvf9MH8Yg4VllDVF7dFhYf5e92pvp63x3uLYKon+I6TpDjVu6XhKOjlrFVQXFplcHs/mHNyOie/mLxq6t3xUeH+XnaH5Dj6dEwipcK93b7wbhGjereEhoJeXMNay9HjJ7zhnHfy3u6Tde7gckp19e7EuEjvB3ESoulyTgsurbKSducAAAwjSURBVPTBnJN3nMRG6RZBaf4U9NLslZdbcgtK2H+siAN5Rew/VsyBY8XszyviwDHneV4RufkllFZxj2B4mKFVfJS/RNIpJb7a4G4VH6V6t7iOgl5CxjcolS+oDxwrZv+xIvb7w7yIA3neD+5UFeDJ8VGktYihdYtoLmzjCQru1IALl0lxUYSp3i1nMQW9NDhfCWW/L7idwPb1vvc7oX4gr6jKO06S4iJp3SKGtBYxfKe1h7QW0d7nHm+ot24RQ0qC7jQRqSsFvdSZb4yTwHKJL8yDp4urHBK2ZWwkaR5vUPc/N57WLWJo7TxPaxFNmieGVE80MZGqe4s0JAW9AJBfXHqy9+3viQeUVPK8y4pOVA5wT3SEv9f93Y6t/KHt63239niDXAEuEhoKepcrLCkNKqHk5J0McV/ve/+xoipHGIyLCqdNC28v+6L0RH9wpzk98bQWMaR5ojWyoEgzZ2xVn+yoRnx8vC0oKGjE5khdFZ0oC+pp+y5eBob3gWPF5BVXvv87JjIsqKfd2rmgmRb0PIYEBbhIgzDGFFpr40N2fAV981JcWua/UOkP74Dg9vXMqxqwKioizNvr9sQElE8CSigtokn1xOiDOyJNLNRBry5bEykpLScnv7jChcyTJRRfSeVwYeVv2IkMN/7e9nmpCQw6L9m5rTDGf3GzdYtofbOOiFRJQX+aSsvKOZhf4u9p788rJifgNkJfrzy3oKTStuFhhjSn1t2+VRx9OybR2umFB5ZQEmMjdR+4iJwyBX01ysotufnFtd5GeDC/uNIAVmEGUhK8Qd0uMYZe7ROdAPfOS3V64cnx+iCPiDS+sy7oy8sthwpLKt1GGPghHl8ppeKHMY2B5Phof2D3TG9Jqi/AA+rhyQnRGnlQROrMGDMbGAkcsNZ2d+a1AuYDHYEdwE+stYeNtz77BHAVUAiMt9aurnH/brkYa63lcOGJqj+J6ZRUDjgBXt3H6VMD6t2BtxH6SikpCdFEhuvTmCJSP7VdjDXGXArkA38PCPrHgEPW2keNMVOAJGvtb40xVwF34Q36/sAT1tr+NR2/2ffofR+nP1Dx/u8Kn87MySuu8guLE+Mi/XehdE5L8d9G2LpFtP+CZqo+Ti8iIWSt/cAY07HC7NHAYGd6DrAM+K0z/+/W20tfYYxJNMa0tdburW7/IQt6ay15xaVBve4DFW8jdEK8qo/Tt4iJ8Pe0+3dq5R/cKvCecH2cXkTOYK0Dwnsf0NqZbgfsClhvtzMvtEH/8opv2H6wwB/ivl54VeOBJ/g+Tu+JoU/7JH8JJfA2wjRPjMYBF5EzSYQxJjvg+Sxr7ay6bmyttcaYutfZKx78VDesj5dXfMM3uYW0aekN7B7piVweUPtuHXBPuD5OLyIuVGqt7VvPbfb7SjLGmLbAAWf+HiAjYL10Z161miRV3/xlJtERYfowj4hI3S0CxgGPOj8XBsyfYIyZh/di7NGa6vPQREGvOrmISPWMMa/gvfCaYozZDfweb8C/aoy5BfgG+Imz+jt477jZgvf2yp/Vun+33F4pItJchXqsG91TKCLicgp6ERGXU9CLiLicgl5ExOUU9CIiLqegFxFxOQW9iIjLKehFRFxOQS8i4nIKehERl1PQi4i4nIJeRMTlFPQiIi6noBcRcTkFvYiIyynoRURcTkEvIuJyCnoREZdT0IuIuJyCXkTE5RT0IiIup6AXEXE5Bb2IiMsp6EVEXE5BLyLicgp6ERGXU9CLiLicgl5ExOUU9CIiLqegFxFxuYgmOcq7U2Df2iY5lIhIg2vTA658NNStOGXq0YuIuJyx1tZ55fj4eFtQUNCIzRERcR9jTKG1Nr6G5cOBJ4Bw4DlrbYO+fVCPXkQkhIwx4cDTwJVAV+A6Y0zXhjyGgl5EJLT6AVustdustSXAPGB0Qx5AQS8i0vgijDHZAY/bApa1A3YFPN/tzGu4gzfkzkREpEql1tq+oTq4evQiIqG1B8gIeJ7uzGswCnoRkdD6DOhsjOlkjIkCrgUWNeQBVLoREQkha22pMWYC8B+8t1fOttaub8hj6D56EZFGVtt99I1+/PoEvTGmHDh+iseKAEpPcdvGpHbVj9pVP2pX/bi1XbHW2pCVyusV9Kd1IGOyQ3nVuTpqV/2oXfWjdtWP2tU4dDFWRMTlFPQiIi7XlEE/qwmPVR9qV/2oXfWjdtWP2tUImqxGLyIioaHSjYiIyynoRURcrkGC3hgz3BjzlTFmizFmShXLo40x853lnxpjOgYs+50z/ytjzLCGaE8d2zTZGLPBGLPGGLPYGNMhYFmZMeYL59GgH0WuY9vGG2NyAtrw84Bl44wxXzuPcU3crhkBbdpsjDkSsKxRzpkxZrYx5oAxZl01y40x5kmnzWuMMb0DljXmuaqtXdc77VlrjPnYGHNRwLIdzvwvjDHZTdyuwcaYowF/qwcCltX492/kdv0moE3rnNdTK2dZY56vDGPMUicL1htjJlWxTkheYw3KWntaD7wf2d0KnAtEAV8CXSuscyfwjDN9LTDfme7qrB8NdHL2E95EbboMiHOm7/C1yXmef7ptOM22jQeeqmLbVsA252eSM53UVO2qsP5deD+q3ajnDLgU6A2sq2b5VcC7gAEGAJ829rmqY7sG+Y6H9wslPg1YtgNICdH5Ggy8fbp//4ZuV4V1rwaWNNH5agv0dqY9wOYq/j2G5DXWkI+G6NHXZdD80cAcZ3oBMMQYY5z586y1xdba7cAWZ3+N3iZr7VJrbaHzdAXeEeOawul8ycAw4H1r7SFr7WHgfWB4iNp1HfBKAx27WtbaD4BDNawyGvi79VoBJBpj2tK456rWdllrP3aOC034+qrD+apOo375RT3b1SSvLQBr7V5r7WpnOg/YSOWx4EPyGmtIDRH0dRk037+OtbYUOAok13HbxmpToFvw/o/tE2O8Xw6wwhhzTQO051Ta9iPnbeICY4xvCNPG/IKCOu/bKXN1ApYEzG7Mc1aT6trd6F/mUA8VX18WeM8Ys8oEfwFFUxlojPnSGPOuMaabM69ZnC9jTBzesHw9YHaTnC/jLSn3Aj6tsOhMeI3V6KwfvdIYcwPQF/hewOwO1to9xphzgSXGmLXW2q1N2Ky3gFestcXGmNvxvhv6fhMevzbXAgustWUB80J9zpolY8xleIP+koDZlzjnKg143xizyenxNoXVeP9W+caYq4A3gc5NdOy6uBpYbq0N7P03+vkyxiTg/c/lbmvtsYbcd3PQED36ugya71/HGBMBtARy67htY7UJY8zlwL3AKGttsW++tXaP83MbsAzv//INpda2WWtzA9rzHNCnrts2ZrsCXEuFt9aNfM5qUl27G/3LHGpjjOmJ9+832lqb65sfcK4OAP+kYcqVdWKtPWatzXem3wEijTEpNIPz5ajptdUo58sYE4k35Odaa9+oYpVm+xqrs9Mt8uN9V7AN71t530WcbhXW+SXBF2Nfdaa7EXwxdhsNczG2Lm3qhffiU+cK85OAaGc6Bfiahr0oVZe2tQ2Y/gGwwp68+LPdaWOSM92qqdrlrHch3otjpgnPWUeqv7g4guALZSsb+1zVsV3t8V5zGlRhfjzgCZj+GBjehO1q4/vb4Q3Mnc65q9Pfv7Ha5SxvibeOH99U58v53f8OPF7DOiF7jTXY79lAJ+sqvFertwL3OvMexttTBogBXnNe+CuBcwO2vdfZ7ivgygb8A9bWpv8C+4EvnMciZ/4gYK3zQl8L3NLgJ732tv0JWO+0YSlwYcC2NzvncQvws6Zsl/P8QeDRCts12jnD27vbC5zAWwO9BfgF8AtnuQGedtq8FujbROeqtnY9BxwOeH1lO/PPdc7Tl87f+N4mbteEgNfWCgL+I6rq799U7XLWGY/35ozA7Rr7fF2C9xrAmoC/1VXN4TXWkA8NgSAi4nL6ZKyIiMsp6EVEXE5BLyLicgp6ERGXU9CLiLicgl5ExOUU9CIiLqeglzOaMaajMWajMeZZZzzx94wxsaFul0hzoqAXN+gMPG2t7QYcAX4U4vaINCsKenGD7dbaL5zpVXjHVBERh4Je3KA4YLoMDb8tEkRBLyLicgp6ERGX0+iVIiIupx69iIjLKehFRFxOQS8i4nIKehERl1PQi4i4nIJeRMTlFPQiIi73/wHiXj9egA01dQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6wOpWfUgqv5"
      },
      "source": [
        "### Print out the policy and model to see the results of training in detail…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzC2fFaNgr9I",
        "outputId": "d27f5d66-5dbb-48e3-bd38-de53dff8e8f8"
      },
      "source": [
        "import pprint\n",
        "\n",
        "policy = agent.get_policy()\n",
        "model = policy.model\n",
        "\n",
        "pprint.pprint(model.variables())\n",
        "pprint.pprint(model.value_function())\n",
        "\n",
        "print(model.base_model.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(4, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(4, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 2) dtype=float32>,\n",
            " <tf.Variable 'default_policy/fc_out/bias:0' shape=(2,) dtype=float32>,\n",
            " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
            " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
            "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "observations (InputLayer)       [(None, 4)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "fc_1 (Dense)                    (None, 256)          1280        observations[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "fc_value_1 (Dense)              (None, 256)          1280        observations[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "fc_out (Dense)                  (None, 2)            514         fc_2[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 134,915\n",
            "Trainable params: 134,915\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjU5Dr6g_p3"
      },
      "source": [
        "### Model Rollout\n",
        "\n",
        "Once we have trained a policy, we deploy it in the environment.\n",
        "\n",
        "A 'Rollout' is the application of the trained policy to the environment. This is, for a given state, the policy function will output the best action to take.\n",
        "\n",
        "****************************************************************************\n",
        "\n",
        "WARNING: The rllib rollout command discussed next won't work in a cloud environment, because it attempts to pop up a window.\n",
        "\n",
        "\n",
        "https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation\n",
        "\n",
        "***************************************************************************\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "Next we'll use the RLlib rollout CLI, to evaluate the trained policy.\n",
        "\n",
        "This visualizes the CartPole agent operating within the simulation: moving the cart left or right to avoid having the pole fall over.\n",
        "\n",
        "We'll use the last saved checkpoint, checkpoint_10 (or whatever you set for N_ITER above) for the rollout, evaluated through 2000 steps.\n",
        "\n",
        "    Notes:\n",
        "\n",
        "        If you changed checkpoint_root above to be different than tmp/ppo/cart, then change it here, too. Note that bugs in variable substitution in Jupyter notebooks, we can't use variables in the next cell, unfortunately.\n",
        "        If you changed the model parameters, specifically the fcnet_hiddens array in the config object above, make the same change here.\n",
        "\n",
        "You may need to make one more modification, depending on how you are running this tutorial:\n",
        "\n",
        "    Running on your laptop? - Remove the line --no-render.\n",
        "    Running on the Anyscale Service? The popup windows that would normally be created by the rollout can't be viewed in this case. Hence, the --no-render flag suppresses them. The code cell afterwords provides a sample video. You can try adding --video-dir tmp/ppo/cart, which will generate MP4 videos, then download them to view them. Or copy the Video cell below and use it to view the movies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kezNVWhRiF"
      },
      "source": [
        "# On the command line, run the following:\n",
        "\n",
        "#!rllib rollout\\\n",
        "# tmp/ppo/cart/checkpoint_10/checkpoint-10 \\\n",
        "#    --config \"{\\\"env\\\": \\\"CartPole-v1\\\", \\\"model\\\": {\\\"fcnet_hiddens\\\": [100, 50]}}\" \\\n",
        "#    --run PPO \\\n",
        "#     --steps 2000"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hofaz0-9GOa"
      },
      "source": [
        "### Tensorboard results\n",
        "Note: one can also use WandB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYBnmkXI9J2P"
      },
      "source": [
        "#From command line:\n",
        "#tensorboard - logdir=$HOME/ray_results/"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvi_08jVhwsQ"
      },
      "source": [
        "### Shut down the service"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkVVwtL6hw8k"
      },
      "source": [
        "ray.shutdown()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_ZX0ceW6Xh"
      },
      "source": [
        "## Ray Paralellization\n",
        "\n",
        "https://github.com/anyscale/academy/blob/64b5b7d149d1dfd3883948bbde0a247b57fbef0c/ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb\n",
        "\n",
        "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a\n",
        "\n",
        "TL,DR:\n",
        "\n",
        "RAY basically triggers several ROLLOUTS (i.e. episodes) at the same time (by distributing it to many “WORKERS”) so the whole thing runs faster. \"Rollout workers\" collect data in mini batches, then a central worker aggregates these into a single batch of data.\n",
        "\n",
        "LONG ANSWER: https://arxiv.org/pdf/1712.09381.pdf\n",
        "\n",
        "Ray implements a \"centralized parallelization\" where there is a *central scheduler* that sends instructions to the *remote workers*. These workers can, for example, explore the environment and collect rewards (distributed sampling), then the centralized agent will optimize the policy, update parameters, handle the replay buffer, etc.\n",
        "\n",
        "More on Ray Parallelization, at the end of:\n",
        "\n",
        "https://github.com/anyscale/academy/blob/64b5b7d149d1dfd3883948bbde0a247b57fbef0c/ray-rllib/02-Introduction-to-RLlib.ipynb\n",
        ".\n",
        "\n",
        ".\n",
        "### Parallelization parameters:\n",
        "\n",
        "```\n",
        "num_workers -  is the number of actors that the agent will create. It sets the number of CPU processors for parallelization, which determines the degree of parallelism that will be used. In a cluster, these actors will be spread over the available nodes.\n",
        "\n",
        "model  - contains a dictionary of parameters describing the neural net used to parameterize the policy. The fcnet_hiddens parameter is a list of the sizes of the hidden layers. Here, we have two hidden layers of size 100, each.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jqXw-yrK1o"
      },
      "source": [
        "In short, the entire Ray code for PPO is like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA62frf6rIwo",
        "outputId": "baacbdd9-e307-4a70-fda3-cc53aa2f20c0"
      },
      "source": [
        "import ray\n",
        "from ray.rllib import agents\n",
        "\n",
        "#Initialize service and pass the number of resources available\n",
        "ray.init(num_cpus = 1,\n",
        "         num_gpus = 0,\n",
        "         ignore_reinit_error = True)\n",
        "\n",
        "\n",
        "config = {'gamma': 0.9,\n",
        "          'lr': 1e-2,\n",
        "          'num_workers':0, #if we set this to 1, we will pin one worker per core, if we set it to 0, it will utilize several workers per core (workers = process)\n",
        "          'train_batch_size': 1000,\n",
        "          'model': {\n",
        "              'fcnet_hiddens': [128, 128]\n",
        "          }}\n",
        "\n",
        "trainer = agents.ppo.PPOTrainer(env='CartPole-v0', config=config)\n",
        "results = trainer.train()   "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:07:46,661\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
            "2021-10-29 18:07:50,177\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-29 18:07:50,180\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkCnl2QosyH2"
      },
      "source": [
        "### Additional parameters we can pass to the config dict\n",
        "\n",
        "If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger sgd_minibatch_size, a smaller num_sgd_iter, or a larger num_workers.\n",
        "\n",
        "```\n",
        "num_sgd_iter -  is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO, for each minibatch (\"chunk\") of training data. Using minibatches is more efficient than training with one record at a time.\n",
        "\n",
        "sgd_minibatch_size  - is the SGD minibatch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
        "\n",
        "num_cpus_per_worker  - when set to 0 prevents Ray from pinning a CPU core to \n",
        "each worker, which means we could run out of workers in a constrained environment like a laptop or a cloud VM.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBvGCbaFXDdv"
      },
      "source": [
        "config = ppo.DEFAULT_CONFIG.copy()              # PPO's default configuration. \n",
        "config[\"log_level\"] = \"WARN\"                    # Suppress too many messages, but try \"INFO\" to see what can be printed.\n",
        "\n",
        "# Other settings we might adjust:\n",
        "config[\"num_workers\"] = 1                       # Use > 1 for using more CPU cores, including over a cluster\n",
        "config[\"num_sgd_iter\"] = 10                     # Number of SGD (stochastic gradient descent) iterations per training minibatch.\n",
        "                                                # I.e., for each minibatch of data, do this many passes over it to train. \n",
        "config[\"sgd_minibatch_size\"] = 250              # The amount of data records per minibatch\n",
        "config[\"model\"][\"fcnet_hiddens\"] = [100, 50]    # Neural network with two hidden layers, the list contains the number of weights on each layer\n",
        "config[\"num_cpus_per_worker\"] = 0               # This avoids running out of resources in the notebook environment when this cell is re-executed"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNyu5Bp_nxjD"
      },
      "source": [
        "With the above configs, we can train again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKsEe1SLn0uw",
        "outputId": "dbd78ad9-d5ad-450b-c070-e578bc8b06ba"
      },
      "source": [
        "agent = ppo.PPOTrainer(config, env=SELECT_ENV)\n",
        "\n",
        "results = []\n",
        "episode_data = []\n",
        "episode_json = []\n",
        "\n",
        "for n in range(N_ITER):\n",
        "    result = agent.train()\n",
        "    results.append(result)\n",
        "    \n",
        "    episode = {'n': n, \n",
        "               'episode_reward_min': result['episode_reward_min'], \n",
        "               'episode_reward_mean': result['episode_reward_mean'], \n",
        "               'episode_reward_max': result['episode_reward_max'],  \n",
        "               'episode_len_mean': result['episode_len_mean']}\n",
        "    \n",
        "    episode_data.append(episode)\n",
        "    episode_json.append(json.dumps(episode))\n",
        "    file_name = agent.save(CHECKPOINT_ROOT)\n",
        "    \n",
        "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}. Checkpoint saved to {file_name}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:08:00,494\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-29 18:08:00,504\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0: Min/Mean/Max reward:   9.0000/ 23.6095/ 71.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000001/checkpoint-1\n",
            "  1: Min/Mean/Max reward:   9.0000/ 31.0388/115.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000002/checkpoint-2\n",
            "  2: Min/Mean/Max reward:  12.0000/ 40.3000/147.0000. Checkpoint saved to tmp/ppo/cart/checkpoint_000003/checkpoint-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_Yy_nm_vs5"
      },
      "source": [
        "### Load from Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiA9hZ_H6795",
        "outputId": "7800172c-d268-415b-e4cd-35814c9e7d83"
      },
      "source": [
        "# Bring the model config\n",
        "trained_config = config.copy()\n",
        "\n",
        "# Load trained model\n",
        "test_agent = ppo.PPOTrainer(trained_config, SELECT_ENV) #initialize object\n",
        "test_agent.restore(file_name)  #above we have defined: file_name = agent.save(CHECKPOINT_ROOT)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:08:28,641\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
            "2021-10-29 18:08:28,645\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            "2021-10-29 18:08:28,734\tINFO trainable.py:394 -- Restored on 172.28.0.2 from checkpoint: tmp/ppo/cart/checkpoint_000003/checkpoint-3\n",
            "2021-10-29 18:08:28,739\tINFO trainable.py:401 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 20.28170680999756, '_episodes_total': 396}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB_7yZOiBYl3"
      },
      "source": [
        "Example of rollout: \n",
        "\n",
        "Reuse the trained policy to act in an environment\n",
        "The line: `test_agent.compute_action(state)` uses the trained policy to pick an action given the state.\n",
        "\n",
        "The reward received should match the training reward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTk2iZfKBctW",
        "outputId": "09189a67-5e8b-4492-cde2-ab688a352f09"
      },
      "source": [
        "env   = gym.make(SELECT_ENV)\n",
        "state = env.reset()\n",
        "done  = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while not done:\n",
        "  action = test_agent.compute_single_action(state) #gets the next action given a state\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  cumulative_reward += reward\n",
        "\n",
        "print(cumulative_reward)  \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k94AfEtXi_QT"
      },
      "source": [
        "### Use Ray Tune for the Parameters \n",
        "Now we will use DQN\n",
        "and Ray Tune runner to train the algo\n",
        "\n",
        "https://www.codeproject.com/Articles/5271939/Cartpole-The-Hello-World-of-Reinforcement-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "x2iXOaXCoGx1",
        "outputId": "71647f3c-6bec-49ee-e2f2-4c9254e1e5f7"
      },
      "source": [
        "from ray import tune\n",
        "from ray.rllib.agents.dqn import DQNTrainer\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.progress_reporter import JupyterNotebookReporter\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(\n",
        "    ignore_reinit_error=True\n",
        ")\n",
        "\n",
        "ENV = 'CartPole-v0'\n",
        "TARGET_REWARD = 195  #it stops when this reward has been achieved\n",
        "TRAINER = DQNTrainer\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "#Stopping criteria\n",
        "stop_dict ={\"training_iteration\": 3,\n",
        "            \"timesteps_total\"   : 5,\n",
        "            \"episode_reward_mean\": TARGET_REWARD # stop as soon as we \"solve\" the environment            \n",
        "            }  \n",
        "\n",
        "# Parameters for the trainer function - if we use PPO, we can add the Net layers here as above\n",
        "config_dict = { \"env\": ENV,\n",
        "                \"num_workers\": 0,  # run in a single process\n",
        "                \"num_gpus\": 0\n",
        "                }\n",
        "\n",
        "# Runner\n",
        "analysis =  tune.run(\n",
        "              TRAINER,\n",
        "              stop  = stop_dict,\n",
        "              config= config_dict,\n",
        "              progress_reporter=JupyterNotebookReporter(overwrite=False),\n",
        "              verbose=2 #can be changed\n",
        "          )"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:08:32,221\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects<br>Result logdir: /root/ray_results/DQN_2021-10-29_18-08-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>DQN_CartPole-v0_3b195_00000</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 0/0 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects<br>Result logdir: /root/ray_results/DQN_2021-10-29_18-08-34<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>DQN_CartPole-v0_3b195_00000</td><td>RUNNING </td><td>     </td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=686)\u001b[0m 2021-10-29 18:08:39,497\tINFO trainer.py:741 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "\u001b[2m\u001b[36m(pid=686)\u001b[0m 2021-10-29 18:08:39,497\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            "\u001b[2m\u001b[36m(pid=686)\u001b[0m 2021-10-29 18:08:39,497\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=686)\u001b[0m 2021-10-29 18:08:41,167\tWARNING deprecation.py:39 -- DeprecationWarning: `ReplayBuffer(size)` has been deprecated. Use `ReplayBuffer(capacity)` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(pid=686)\u001b[0m 2021-10-29 18:08:41,168\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial DQN_CartPole-v0_3b195_00000 reported episode_reward_max=55.0,episode_reward_min=9.0,episode_reward_mean=18.826923076923077,episode_len_mean=18.826923076923077,episode_media={},episodes_this_iter=52,policy_reward_min={},policy_reward_max={},policy_reward_mean={},custom_metrics={},sampler_perf={'mean_raw_obs_processing_ms': 0.19719598295686253, 'mean_inference_ms': 1.7682612835467757, 'mean_action_processing_ms': 0.08187927566208208, 'mean_env_wait_ms': 0.08790095250208779, 'mean_env_render_ms': 0.0},off_policy_estimator={},num_healthy_workers=0,agent_timesteps_total=1000,timers={'load_time_ms': 0.413, 'load_throughput': 77448.198, 'learn_time_ms': 169.795, 'learn_throughput': 188.463},info={'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257, 'mean_q': 0.09098598, 'min_q': -0.82689464, 'max_q': 0.6952868, 'mean_td_error': -0.7997943, 'model': {}}, 'td_error': array([-0.44876218, -0.92450964, -0.7006046 , -0.7936419 , -1.1696949 ,\n",
            "       -0.6950869 , -0.67704535, -1.1529248 , -0.6753204 , -1.5944055 ,\n",
            "       -1.0973612 , -1.0925492 , -0.25241715, -0.6056608 , -0.57625467,\n",
            "       -0.5159699 , -0.40552348, -0.6224861 , -0.5196456 , -0.89672333,\n",
            "       -0.7332827 , -0.29262352, -0.9521534 , -1.1042322 , -1.1088061 ,\n",
            "       -0.8299983 , -0.9224026 , -0.3934077 , -0.9324771 , -1.1438923 ,\n",
            "       -0.44795159, -1.3156039 ], dtype=float32), 'custom_metrics': {}}}, 'num_steps_sampled': 1000, 'num_agent_steps_sampled': 1000, 'num_steps_trained': 32, 'num_agent_steps_trained': 32, 'last_target_update_ts': 1000, 'num_target_updates': 1},perf={'cpu_util_percent': 62.6, 'ram_util_percent': 17.275} with parameters={'env': 'CartPole-v0', 'num_workers': 0, 'num_gpus': 0}. This trial completed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects<br>Result logdir: /root/ray_results/DQN_2021-10-29_18-08-34<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>DQN_CartPole-v0_3b195_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.67267</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 18.8269</td><td style=\"text-align: right;\">                  55</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           18.8269</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-29 18:08:44,082\tINFO tune.py:617 -- Total run time: 9.97 seconds (9.67 seconds for the tuning loop).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69r_Vg6rHQfE"
      },
      "source": [
        "Analyse the training results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Qw_7KpFHVDG",
        "outputId": "8deb9418-8b00-4ec7-93d4-9dba23a2da41"
      },
      "source": [
        "df = analysis.dataframe()\n",
        "df"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_len_mean</th>\n",
              "      <th>episodes_this_iter</th>\n",
              "      <th>num_healthy_workers</th>\n",
              "      <th>timesteps_total</th>\n",
              "      <th>agent_timesteps_total</th>\n",
              "      <th>done</th>\n",
              "      <th>episodes_total</th>\n",
              "      <th>training_iteration</th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>date</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>time_this_iter_s</th>\n",
              "      <th>time_total_s</th>\n",
              "      <th>pid</th>\n",
              "      <th>hostname</th>\n",
              "      <th>node_ip</th>\n",
              "      <th>time_since_restore</th>\n",
              "      <th>timesteps_since_restore</th>\n",
              "      <th>iterations_since_restore</th>\n",
              "      <th>trial_id</th>\n",
              "      <th>hist_stats/episode_reward</th>\n",
              "      <th>hist_stats/episode_lengths</th>\n",
              "      <th>sampler_perf/mean_raw_obs_processing_ms</th>\n",
              "      <th>sampler_perf/mean_inference_ms</th>\n",
              "      <th>sampler_perf/mean_action_processing_ms</th>\n",
              "      <th>sampler_perf/mean_env_wait_ms</th>\n",
              "      <th>sampler_perf/mean_env_render_ms</th>\n",
              "      <th>timers/load_time_ms</th>\n",
              "      <th>timers/load_throughput</th>\n",
              "      <th>timers/learn_time_ms</th>\n",
              "      <th>timers/learn_throughput</th>\n",
              "      <th>info/num_steps_sampled</th>\n",
              "      <th>info/num_agent_steps_sampled</th>\n",
              "      <th>info/num_steps_trained</th>\n",
              "      <th>info/num_agent_steps_trained</th>\n",
              "      <th>info/last_target_update_ts</th>\n",
              "      <th>info/num_target_updates</th>\n",
              "      <th>perf/cpu_util_percent</th>\n",
              "      <th>perf/ram_util_percent</th>\n",
              "      <th>info/learner/default_policy/td_error</th>\n",
              "      <th>info/learner/default_policy/learner_stats/cur_lr</th>\n",
              "      <th>info/learner/default_policy/learner_stats/mean_q</th>\n",
              "      <th>info/learner/default_policy/learner_stats/min_q</th>\n",
              "      <th>info/learner/default_policy/learner_stats/max_q</th>\n",
              "      <th>info/learner/default_policy/learner_stats/mean_td_error</th>\n",
              "      <th>config/env</th>\n",
              "      <th>config/num_gpus</th>\n",
              "      <th>config/num_workers</th>\n",
              "      <th>logdir</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>55.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>18.826923</td>\n",
              "      <td>18.826923</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>True</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>04a4441f46824874afefe243ddca3bff</td>\n",
              "      <td>2021-10-29_18-08-43</td>\n",
              "      <td>1635530923</td>\n",
              "      <td>2.672673</td>\n",
              "      <td>2.672673</td>\n",
              "      <td>686</td>\n",
              "      <td>78972b9d4096</td>\n",
              "      <td>172.28.0.2</td>\n",
              "      <td>2.672673</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3b195_00000</td>\n",
              "      <td>[11.0, 27.0, 37.0, 17.0, 14.0, 16.0, 19.0, 38....</td>\n",
              "      <td>[11, 27, 37, 17, 14, 16, 19, 38, 12, 18, 25, 1...</td>\n",
              "      <td>0.197196</td>\n",
              "      <td>1.768261</td>\n",
              "      <td>0.081879</td>\n",
              "      <td>0.087901</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.413</td>\n",
              "      <td>77448.198</td>\n",
              "      <td>169.795</td>\n",
              "      <td>188.463</td>\n",
              "      <td>1000</td>\n",
              "      <td>1000</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>1000</td>\n",
              "      <td>1</td>\n",
              "      <td>62.6</td>\n",
              "      <td>17.275</td>\n",
              "      <td>[-0.44876218 -0.92450964 -0.7006046  -0.793641...</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.090986</td>\n",
              "      <td>-0.826895</td>\n",
              "      <td>0.695287</td>\n",
              "      <td>-0.799794</td>\n",
              "      <td>CartPole-v0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>/root/ray_results/DQN_2021-10-29_18-08-34/DQN_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   episode_reward_max  ...                                             logdir\n",
              "0                55.0  ...  /root/ray_results/DQN_2021-10-29_18-08-34/DQN_...\n",
              "\n",
              "[1 rows x 52 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7BqN_E5oW_s"
      },
      "source": [
        "## Next Steps:\n",
        "Custom Gym environments and RLLIB\n",
        "\n",
        "https://medium.com/distributed-computing-with-ray/anatomy-of-a-custom-environment-for-rllib-327157f269e5\n",
        "\n",
        "https://developpaper.com/ray-and-rllib-for-fast-parallel-reinforcement-learning/\n",
        "\n",
        "https://towardsdatascience.com/ray-and-rllib-for-fast-and-parallel-reinforcement-learning-6d31ee21c96c\n",
        "\n",
        "Example of custom environments and reward shaping\n",
        "\n",
        "https://colab.research.google.com/github/valin1/rllib-tutorial/blob/master/RLlib_Tutorial.ipynb\n",
        "\n",
        "https://colab.research.google.com/github/ray-project/tutorial/blob/master/rllib_exercises/rllib_colab.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6QNegJQobJ6"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}